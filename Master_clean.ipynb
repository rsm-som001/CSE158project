{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction & Dataset Overview\n",
    "\n",
    "In this assignment, we work with a large-scale Reddit submissions dataset containing over 130,000 posts.\n",
    "\n",
    "Each row represents a single Reddit submission, with metadata such as:\n",
    "\n",
    "title ‚Äì the text of the post\n",
    "\n",
    "subreddit ‚Äì the community it was posted in\n",
    "\n",
    "number_of_upvotes / number_of_downvotes ‚Äì voting metrics\n",
    "\n",
    "score ‚Äì Reddit‚Äôs official score (upvotes ‚àí downvotes)\n",
    "\n",
    "total_votes ‚Äì sum of upvotes + downvotes\n",
    "\n",
    "rawtime / unixtime / localtime ‚Äì posting timestamps\n",
    "\n",
    "username ‚Äì poster identity (sparsely available)\n",
    "\n",
    "The goal of this project is to build a predictive model that can estimate a post‚Äôs popularity (score) using interpretable features derived from metadata and text.\n",
    "Before performing any modeling, we begin with a thorough validation of data quality, ensuring that all fields behave according to Reddit‚Äôs definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#image_id</th>\n",
       "      <th>unixtime</th>\n",
       "      <th>rawtime</th>\n",
       "      <th>title</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>number_of_upvotes</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>number_of_downvotes</th>\n",
       "      <th>localtime</th>\n",
       "      <th>score</th>\n",
       "      <th>number_of_comments</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333172e+09</td>\n",
       "      <td>2012-03-31T12:40:39.590113-07:00</td>\n",
       "      <td>And here's a downvote.</td>\n",
       "      <td>63470.0</td>\n",
       "      <td>rmqjs</td>\n",
       "      <td>32657.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>30813.0</td>\n",
       "      <td>1.333198e+09</td>\n",
       "      <td>1844.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>Animates_Everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333178e+09</td>\n",
       "      <td>2012-03-31T14:16:01.093638-07:00</td>\n",
       "      <td>Expectation</td>\n",
       "      <td>35.0</td>\n",
       "      <td>rmun4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.333203e+09</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Gangsta_Raper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333200e+09</td>\n",
       "      <td>2012-03-31T20:18:33.192906-07:00</td>\n",
       "      <td>Downvote</td>\n",
       "      <td>41.0</td>\n",
       "      <td>rna86</td>\n",
       "      <td>32.0</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.333225e+09</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gangsta_Raper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333252e+09</td>\n",
       "      <td>2012-04-01T10:52:10-07:00</td>\n",
       "      <td>Every time I downvote something</td>\n",
       "      <td>10.0</td>\n",
       "      <td>ro7e4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.333278e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gangsta_Raper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333273e+09</td>\n",
       "      <td>2012-04-01T16:35:54.393381-07:00</td>\n",
       "      <td>Downvote &amp;quot;Dies Irae&amp;quot;</td>\n",
       "      <td>65.0</td>\n",
       "      <td>rooof</td>\n",
       "      <td>57.0</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.333298e+09</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gangsta_Raper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #image_id      unixtime                           rawtime  \\\n",
       "0          0  1.333172e+09  2012-03-31T12:40:39.590113-07:00   \n",
       "1          0  1.333178e+09  2012-03-31T14:16:01.093638-07:00   \n",
       "2          0  1.333200e+09  2012-03-31T20:18:33.192906-07:00   \n",
       "3          0  1.333252e+09         2012-04-01T10:52:10-07:00   \n",
       "4          0  1.333273e+09  2012-04-01T16:35:54.393381-07:00   \n",
       "\n",
       "                             title  total_votes reddit_id  number_of_upvotes  \\\n",
       "0           And here's a downvote.      63470.0     rmqjs            32657.0   \n",
       "1                      Expectation         35.0     rmun4               29.0   \n",
       "2                         Downvote         41.0     rna86               32.0   \n",
       "3  Every time I downvote something         10.0     ro7e4                6.0   \n",
       "4   Downvote &quot;Dies Irae&quot;         65.0     rooof               57.0   \n",
       "\n",
       "  subreddit  number_of_downvotes     localtime   score  number_of_comments  \\\n",
       "0     funny              30813.0  1.333198e+09  1844.0               622.0   \n",
       "1  GifSound                  6.0  1.333203e+09    23.0                 3.0   \n",
       "2  GifSound                  9.0  1.333225e+09    23.0                 0.0   \n",
       "3  GifSound                  4.0  1.333278e+09     2.0                 0.0   \n",
       "4  GifSound                  8.0  1.333298e+09    49.0                 0.0   \n",
       "\n",
       "              username  \n",
       "0  Animates_Everything  \n",
       "1        Gangsta_Raper  \n",
       "2        Gangsta_Raper  \n",
       "3        Gangsta_Raper  \n",
       "4        Gangsta_Raper  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/Users/benjaminma/Desktop/MGTA461+Data/CSE158project/AndrewFile/redditSubmissions.csv.gz\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\",\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Step 1 ‚Äî Data Validation & Integrity Checks\n",
    "\n",
    "Before running any exploratory analysis or building models, it is crucial to verify that the dataset is internally consistent and free of obvious structural issues.\n",
    "\n",
    "We focus on four core checks:\n",
    "\n",
    "#### 1. Score Consistency\n",
    "\n",
    "Reddit defines:\n",
    "\n",
    "score = upvotes ‚àí downvotes\n",
    "\n",
    "We verify that this relationship holds for all posts.\n",
    "\n",
    "#### 2. Non-Negative Vote Counts\n",
    "\n",
    "Upvotes and downvotes are counts that must be zero or positive.\n",
    "Any negative values would indicate corrupted rows.\n",
    "\n",
    "#### 3. Duplicate Posts\n",
    "\n",
    "The column reddit_id uniquely identifies each submission.\n",
    "Duplicates would indicate reposted or corrupted data.\n",
    "\n",
    "#### 4. Missing Values\n",
    "\n",
    "We inspect which fields contain missing values and assess whether they affect modeling.\n",
    "Notably, the username column is highly sparse, but all modeling-relevant columns are essentially complete.\n",
    "\n",
    "These checks ensure the dataset is structurally sound before we move on to EDA and engineered features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEN'S EDA IS RIGHT HERE - CAN REMOVE THIS LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## however many cells needed. Note the structure of the data.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "eda_df = pd.read_csv(\"BenEDA/reddit_prepared.csv\")\n",
    "eda_df[\"post_date\"] = pd.to_datetime(eda_df[\"post_date\"])\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle(\"Reddit Post Analysis Dashboard\", fontsize=16, fontweight=\"bold\", y=1.00)\n",
    "\n",
    "# 1. Best Hours to Post\n",
    "ax1 = axes[0, 0]\n",
    "hour_scores = eda_df.groupby(\"hour\")[\"score\"].mean().sort_index()\n",
    "bars = ax1.bar(hour_scores.index, hour_scores.values, color=\"steelblue\", alpha=0.7)\n",
    "max_hour = int(hour_scores.idxmax())\n",
    "bars[max_hour].set_color(\"red\")\n",
    "bars[max_hour].set_alpha(1.0)\n",
    "ax1.axvline(x=17, color=\"red\", linestyle=\"--\", alpha=0.3, linewidth=2)\n",
    "ax1.axvline(x=2, color=\"orange\", linestyle=\"--\", alpha=0.3, linewidth=2)\n",
    "ax1.set_xlabel(\"Hour of Day\")\n",
    "ax1.set_ylabel(\"Average Score\")\n",
    "ax1.set_title(f\"Best Time to Post: {max_hour}:00 (5pm=17, 2am=2)\")\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 2. Day of Week Pattern\n",
    "ax2 = axes[0, 1]\n",
    "day_names = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "dow_scores = eda_df.groupby(\"day_of_week\")[\"score\"].mean()\n",
    "colors = [\"steelblue\"] * 5 + [\"coral\", \"coral\"]\n",
    "ax2.bar(range(7), dow_scores.values, color=colors, alpha=0.7)\n",
    "ax2.set_xticks(range(7))\n",
    "ax2.set_xticklabels(day_names)\n",
    "ax2.set_ylabel(\"Average Score\")\n",
    "ax2.set_title(\"Weekend vs Weekday\")\n",
    "ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 3. Top Subreddits\n",
    "ax3 = axes[0, 2]\n",
    "top_subs = eda_df[\"subreddit\"].value_counts().head(10)\n",
    "ax3.barh(range(10), top_subs.values, color=\"teal\", alpha=0.7)\n",
    "ax3.set_yticks(range(10))\n",
    "ax3.set_yticklabels(top_subs.index, fontsize=9)\n",
    "ax3.set_xlabel(\"Number of Posts\")\n",
    "ax3.set_title(\"Most Active Subreddits\")\n",
    "ax3.invert_yaxis()\n",
    "ax3.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# 4. Title Length vs Score\n",
    "ax4 = axes[1, 0]\n",
    "sample = eda_df.sample(min(3000, len(eda_df)))\n",
    "ax4.scatter(sample[\"title_length\"], sample[\"score\"], alpha=0.05, s=20, color=\"purple\")\n",
    "ax4.set_xlabel(\"Title Length (characters)\")\n",
    "ax4.set_ylabel(\"Score\")\n",
    "ax4.set_title(\"Title Length Impact\")\n",
    "ax4.set_ylim(0, eda_df[\"score\"].quantile(0.95))\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(eda_df[\"title_length\"].dropna(), eda_df[\"score\"], 1)\n",
    "p = np.poly1d(z)\n",
    "x_trend = np.linspace(eda_df[\"title_length\"].min(), eda_df[\"title_length\"].max(), 100)\n",
    "ax4.plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# 5. Score Distribution\n",
    "ax5 = axes[1, 1]\n",
    "ax5.hist(eda_df[\"score\"], bins=50, color=\"steelblue\", alpha=0.7, edgecolor=\"black\")\n",
    "ax5.axvline(\n",
    "    eda_df[\"score\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {eda_df['score'].mean():.0f}\",\n",
    ")\n",
    "ax5.axvline(\n",
    "    eda_df[\"score\"].median(),\n",
    "    color=\"orange\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Median: {eda_df['score'].median():.0f}\",\n",
    ")\n",
    "ax5.set_xlabel(\"Score\")\n",
    "ax5.set_ylabel(\"Frequency (log scale)\")\n",
    "ax5.set_title(\"Score Distribution\")\n",
    "ax5.set_xlim(0, eda_df[\"score\"].quantile(0.95))\n",
    "ax5.set_yscale(\"log\")\n",
    "ax5.legend()\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "# 6. Correlation Heatmap (Top Features)\n",
    "ax6 = axes[1, 2]\n",
    "key_features = [\n",
    "    \"score\",\n",
    "    \"hour\",\n",
    "    \"day_of_week\",\n",
    "    \"title_length\",\n",
    "    \"has_question_mark\",\n",
    "    \"positive_words\",\n",
    "    \"author_avg_score\",\n",
    "]\n",
    "corr_matrix = eda_df[key_features].corr()\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=1,\n",
    "    ax=ax6,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    ")\n",
    "ax6.set_title(\"Feature Correlations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"- Best hour to post: {max_hour}:00\")\n",
    "print(f\"- Best day: {day_names[int(dow_scores.idxmax())]}\")\n",
    "print(f\"- Average score: {eda_df['score'].mean():.1f}\")\n",
    "print(f\"- Median score: {eda_df['score'].median():.1f}\")\n",
    "print(f\"- Title length correlation: {eda_df['title_length'].corr(eda_df['score']):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "import altair as alt\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD DATA (Polars)\n",
    "# ==========================================================\n",
    "reddit_pl = pl.read_csv(\"BenEDA/reddit_prepared.csv\").with_columns(\n",
    "    pl.col(\"post_date\").str.strptime(pl.Datetime, strict=False)\n",
    ")\n",
    "\n",
    "# Convert once to pandas for plotting\n",
    "reddit_pd = reddit_pl.to_pandas()\n",
    "\n",
    "# ==========================================================\n",
    "# 1. Best Hours to Post\n",
    "# ==========================================================\n",
    "hour_scores = (\n",
    "    reddit_pl.group_by(\"hour\")\n",
    "    .agg(pl.col(\"score\").mean().alias(\"mean_score\"))\n",
    "    .sort(\"hour\")\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "best_hour = int(hour_scores.loc[hour_scores[\"mean_score\"].idxmax(), \"hour\"])\n",
    "\n",
    "p1 = (\n",
    "    alt.Chart(hour_scores)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        x=alt.X(\"hour:O\", title=\"Hour of Day\"),\n",
    "        y=alt.Y(\"mean_score:Q\", title=\"Average Score\"),\n",
    "        color=alt.condition(\n",
    "            alt.datum.hour == best_hour, alt.value(\"red\"), alt.value(\"steelblue\")\n",
    "        ),\n",
    "        tooltip=[\"hour\", \"mean_score\"],\n",
    "    )\n",
    "    .properties(title=f\"Best Hour to Post (Red = {best_hour}:00)\")\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# 2. Day of Week Pattern\n",
    "# ==========================================================\n",
    "dow_scores = (\n",
    "    reddit_pl.group_by(\"day_of_week\")\n",
    "    .agg(pl.col(\"score\").mean().alias(\"mean_score\"))\n",
    "    .sort(\"day_of_week\")\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "dow_scores[\"weekday\"] = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "dow_scores[\"is_weekend\"] = dow_scores[\"weekday\"].isin([\"Sat\", \"Sun\"])\n",
    "\n",
    "p2 = (\n",
    "    alt.Chart(dow_scores)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        x=\"weekday:N\",\n",
    "        y=\"mean_score:Q\",\n",
    "        color=alt.Color(\"is_weekend:N\", scale=alt.Scale(range=[\"steelblue\", \"coral\"])),\n",
    "        tooltip=[\"weekday\", \"mean_score\"],\n",
    "    )\n",
    "    .properties(title=\"Weekend vs Weekday\")\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# 3. Top Subreddits\n",
    "# ==========================================================\n",
    "top_subs = (\n",
    "    reddit_pl.group_by(\"subreddit\")\n",
    "    .count()\n",
    "    .sort(\"count\", descending=True)\n",
    "    .head(10)\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "p3 = (\n",
    "    alt.Chart(top_subs)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        x=\"count:Q\", y=alt.Y(\"subreddit:N\", sort=\"-x\"), tooltip=[\"subreddit\", \"count\"]\n",
    "    )\n",
    "    .properties(title=\"Top 10 Active Subreddits\")\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# 4. Title Length vs Score (Scatter + Trendline)\n",
    "# ==========================================================\n",
    "sample_df = reddit_pd.sample(n=min(3000, len(reddit_pd)))\n",
    "\n",
    "p4_scatter = (\n",
    "    alt.Chart(sample_df)\n",
    "    .mark_point(opacity=0.2, color=\"purple\")\n",
    "    .encode(x=\"title_length:Q\", y=\"score:Q\", tooltip=[\"title_length\", \"score\"])\n",
    ")\n",
    "\n",
    "p4_line = p4_scatter.transform_regression(\"title_length\", \"score\").mark_line(\n",
    "    color=\"red\"\n",
    ")\n",
    "\n",
    "p4 = (p4_scatter + p4_line).properties(title=\"Title Length Impact on Score\")\n",
    "\n",
    "# ==========================================================\n",
    "# 5. Score Distribution\n",
    "# ==========================================================\n",
    "mean_score = reddit_pd[\"score\"].mean()\n",
    "median_score = reddit_pd[\"score\"].median()\n",
    "\n",
    "p5 = (\n",
    "    alt.Chart(reddit_pd)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        x=alt.X(\"score:Q\", bin=alt.Bin(maxbins=50)), y=\"count()\", tooltip=[\"count()\"]\n",
    "    )\n",
    "    .properties(\n",
    "        title=f\"Score Distribution (Mean={mean_score:.0f}, Median={median_score:.0f})\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# 6. Correlation Heatmap\n",
    "# ==========================================================\n",
    "key_features = [\n",
    "    \"score\",\n",
    "    \"hour\",\n",
    "    \"day_of_week\",\n",
    "    \"title_length\",\n",
    "    \"has_question_mark\",\n",
    "    \"positive_words\",\n",
    "    \"author_avg_score\",\n",
    "]\n",
    "\n",
    "corr = reddit_pd[key_features].corr().stack().reset_index()\n",
    "corr.columns = [\"var1\", \"var2\", \"value\"]\n",
    "\n",
    "p6 = (\n",
    "    alt.Chart(corr)\n",
    "    .mark_rect()\n",
    "    .encode(\n",
    "        x=\"var1:N\",\n",
    "        y=\"var2:N\",\n",
    "        color=alt.Color(\"value:Q\", scale=alt.Scale(scheme=\"redblue\")),\n",
    "        tooltip=[\"var1\", \"var2\", \"value\"],\n",
    "    )\n",
    "    .properties(title=\"Feature Correlations\")\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# FULL DASHBOARD\n",
    "# ==========================================================\n",
    "dashboard = ((p1 | p2 | p3) & (p4 | p5 | p6)).resolve_scale(color=\"independent\")\n",
    "\n",
    "dashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Validation\n",
    "\n",
    "Before building any models, we first validate that the core numeric fields in the dataset are internally consistent and that there are no obvious data quality issues.\n",
    "\n",
    "**Checks performed:**\n",
    "\n",
    "1. **Score consistency**\n",
    "   We verify that the Reddit `score` field matches the definition:\n",
    "      `score` = `number_of_upvotes` - `number_of_downvotes`\n",
    "\n",
    "2. **Non-negative votes**\n",
    "\n",
    "   We confirm that `number_of_upvotes` and `number_of_downvotes` are never negative.  \n",
    "\n",
    "3. **Duplicated posts**\n",
    "\n",
    "   Using `reddit_id` as the unique identifier of a submission, we check for duplicated `reddit_id`s.  \n",
    "\n",
    "4. **Missing values**\n",
    "\n",
    "   Finally, we compute the total number of missing values in the dataset\n",
    "      - The `username` column has 20,260 missing values, but will not be considered in our model\n",
    "      - Every other column only has 1 missing value on the same row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score = upvotes - downvotes\n",
    "score_diff = (\n",
    "    (df[\"number_of_upvotes\"] - df[\"number_of_downvotes\"] - df[\"score\"]).abs().sum()\n",
    ")\n",
    "print(\"Score consistency check (should be 0):\", score_diff)\n",
    "\n",
    "# Non-negative votes\n",
    "\n",
    "neg_votes = ((df[\"number_of_upvotes\"] < 0) | (df[\"number_of_downvotes\"] < 0)).sum()\n",
    "print(\"Number of rows with negative votes (should be 0):\", neg_votes)\n",
    "\n",
    "dup_count = df[\"reddit_id\"].duplicated().sum()\n",
    "print(\"Number of duplicated reddit_id values:\", dup_count)\n",
    "\n",
    "# Missing Values\n",
    "df_nan = df.isna().sum()\n",
    "print(df_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The validation checks show that the dataset is high-quality and suitable for modeling, with a few minor notes:\n",
    "\n",
    "Score consistency holds perfectly for all rows\n",
    "Every post satisfies the Reddit definition score = upvotes ‚àí downvotes.\n",
    "\n",
    "No negative vote counts were found, confirming that the voting data is well-formed.\n",
    "\n",
    "A small number of duplicated reddit_id values exist (93 duplicates)\n",
    "This is less than 0.1% of the dataset and likely corresponds to reposts or duplicate entries.\n",
    "These duplicates can be safely ignored or removed since they do not materially affect modeling.\n",
    "\n",
    "Missing values are extremely limited -->\n",
    "Each modeling-relevant column has only 1 missing value, all occurring in the same row.\n",
    "The username field is heavily missing (20k+ rows) but is not used in prediction.\n",
    "\n",
    "Overall, the dataset is clean and consistent.\n",
    "We can confidently proceed to feature engineering and exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Cleaning & Pre-Processing\n",
    "\n",
    "Before performing exploratory analysis or modeling, we apply a series of data-cleaning steps to ensure that all fields are in a consistent format and ready for feature engineering.\n",
    "\n",
    "Reddit data often includes HTML artifacts, inconsistent types, missing values, and duplicated metadata.\n",
    "This section standardizes those fields.\n",
    "\n",
    "#### Cleaning Steps Performed:\n",
    "##### 1. Type Casting\n",
    "\n",
    "Several fields (e.g., vote counts, timestamps) are loaded as strings.\n",
    "To make them usable, we convert them into numerical and datetime formats:\n",
    "\n",
    "* unixtime ‚Üí numeric\n",
    "\n",
    "* datetime ‚Üí converted using Unix timestamps\n",
    "\n",
    "* total_votes, number_of_upvotes, number_of_downvotes, score ‚Üí cast to float\n",
    "\n",
    "2. Duplicate Post Removal\n",
    "\n",
    "We drop duplicate posts based on reddit_id, since reposts or duplicate lines may appear in the raw dataset.\n",
    "\n",
    "3. Missing Value Cleanup\n",
    "\n",
    "We remove username entirely (20k+ missing values, not used in modeling).\n",
    "\n",
    "All remaining rows with missing data are dropped (only 1 row removed).\n",
    "\n",
    "4. Title Text Normalization\n",
    "\n",
    "Reddit titles frequently contain:\n",
    "\n",
    "* HTML escape sequences (e.g., &quot;, &amp;)\n",
    "\n",
    "* Extra whitespace\n",
    "\n",
    "* Mixed casing\n",
    "\n",
    "We clean titles by:\n",
    "\n",
    "* Unescaping HTML\n",
    "\n",
    "* Lowercasing\n",
    "\n",
    "* Collapsing repeated whitespace\n",
    "\n",
    "* Ensuring all titles are strings\n",
    "\n",
    "This step is essential to prepare the text for TF-IDF vectorization used later in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting\n",
    "df[\"unixtime\"] = pd.to_numeric(df[\"unixtime\"], errors=\"coerce\")\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"unixtime\"], unit=\"s\")\n",
    "df[\"total_votes\"] = df[\"total_votes\"].astype(float)\n",
    "df[\"number_of_upvotes\"] = df[\"number_of_upvotes\"].astype(float)\n",
    "df[\"number_of_downvotes\"] = df[\"number_of_downvotes\"].astype(float)\n",
    "df[\"score\"] = df[\"score\"].astype(float)\n",
    "\n",
    "# Duplicate Handling\n",
    "df = df.drop_duplicates(subset=[\"reddit_id\"])\n",
    "dup_reddit_ids = df[\"reddit_id\"].duplicated().sum()\n",
    "print(\"Number of duplicated reddit_id values:\", dup_reddit_ids)\n",
    "\n",
    "# Missing Values Handling\n",
    "df = df.drop(columns=[\"username\"])\n",
    "df = df.dropna()\n",
    "print(\"Number of NANs:\", df.isna().sum().sum())\n",
    "\n",
    "\n",
    "def clean_title(text):\n",
    "    # 1. Ensure string\n",
    "    s = str(text)\n",
    "    # 2. Unescape HTML entities: &quot; &amp; &lt; &gt; etc.\n",
    "    s = html.unescape(s)\n",
    "    # 3. Strip leading/trailing whitespace\n",
    "    s = s.strip()\n",
    "    # 4. Collapse multiple spaces/newlines into a single space\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # 5. (Optional) lowercase for modeling\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "\n",
    "df[\"title_clean\"] = df[\"title\"].apply(clean_title)\n",
    "df[\"title\"] = df[\"title_clean\"]\n",
    "df.drop(columns=[\"title_clean\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Cleaning Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After applying the cleaning pipeline:\n",
    "\n",
    "* All numeric fields are correctly typed and ready for aggregation, modeling, and feature engineering.\n",
    "\n",
    "* All duplicated reddit_id rows were removed, leaving a unique set of Reddit submissions.\n",
    "\n",
    "* Only 1 row with missing values was dropped, resulting in a nearly complete dataset.\n",
    "\n",
    "* Title text is normalized for NLP-based feature extraction (TF-IDF), improving model performance.\n",
    "\n",
    "* The final dataset is clean, consistent, and prepared for the next stage: Exploratory Data Analysis (EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers in Score Distribution\n",
    "\n",
    "Reddit post scores are extremely heavy-tailed. Even after applying a log-transform, we found that the top 1‚Äì5% of posts had massively disproportionate influence on our model.\n",
    "These extreme values caused:\n",
    "\n",
    "* Instability during model training\n",
    "\n",
    "* Very large error spikes\n",
    "\n",
    "* Poor generalization on typical posts\n",
    "\n",
    "To address this, we remove the top 5% of scores. This is a standard approach when working with highly skewed social-media engagement data.\n",
    "\n",
    "Removing these outliers makes the score distribution far more well-behaved and leads to significantly improved MAE in both log and raw score space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p95 = df[\"score\"].quantile(0.95)\n",
    "df = df[df[\"score\"] <= p95].copy()\n",
    "\n",
    "\n",
    "# this move massively improved our model\n",
    "\n",
    "# I dont have a graph but i'm pretty sure the values towards the top are astronomically large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of Outlier Removal\n",
    "\n",
    "After filtering the top 5% of scores:\n",
    "\n",
    "* The dataset becomes more statistically stable\n",
    "\n",
    "* The log-transformed target behaves normally\n",
    "\n",
    "* Extreme score values no longer dominate the model\n",
    "\n",
    "* Downstream models produce much lower and more interpretable MAE\n",
    "\n",
    "This step was crucial ‚Äî without it, even strong models (e.g., boosted trees) struggled.\n",
    "After outlier removal, model performance improved dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "To improve model performance, we engineered several new features capturing temporal patterns, linguistic structure, and semantic cues in the title text.\n",
    "Below we outline each feature category and show the transformations step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Features Extracted from Timestamps\n",
    "\n",
    "Reddit engagement is strongly influenced by when a post is made (hour, weekday, month).\n",
    "We extract several time-based predictors from the datetime column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "df[\"dayofweek\"] = df[\"datetime\"].dt.dayofweek\n",
    "df[\"month\"] = df[\"datetime\"].dt.month\n",
    "df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Structure Features\n",
    "\n",
    "These features capture simple but meaningful signals from title text, such as length, wordiness, and whether a question or exclamation is being asked.\n",
    "\n",
    "We compute:\n",
    "\n",
    "* Total character length\n",
    "\n",
    "* Word count\n",
    "\n",
    "* If title contains a ?\n",
    "\n",
    "* If title contains a !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title\"] = df[\"title\"].astype(str).str.strip()\n",
    "df[\"title_length\"] = df[\"title\"].apply(len)\n",
    "df[\"word_count\"] = df[\"title\"].apply(lambda x: len(x.split()))\n",
    "df[\"is_question\"] = df[\"title\"].str.contains(\"?\", regex=False).astype(int)\n",
    "df[\"is_exclamation\"] = df[\"title\"].str.contains(\"!\", regex=False).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subreddit-Level Statistics (Omitted to Avoid Target Leakage)\n",
    "\n",
    "We considered adding subreddit-level aggregate features (e.g., avg score per subreddit), but this directly leaks target information from the training set into the features.\n",
    "\n",
    "Therefore, this block is intentionally commented out and not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit_stats = df.groupby('subreddit')['score'].agg(\n",
    "#     subreddit_mean_score='mean',\n",
    "#     subreddit_post_count='count'\n",
    "# ).reset_index()\n",
    "# df = df.merge(subreddit_stats, on='subreddit', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Features Using VADER\n",
    "\n",
    "Engagement on Reddit is often influenced not just by what a post says but how it is expressed.\n",
    "Emotional or provocative wording can drive significantly higher voting activity. To capture this effect, we extract sentiment features from the post title using the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analyzer.\n",
    "\n",
    "VADER is specifically designed for short, informal, and emotion-heavy text making it an ideal fit for Reddit titles, which often include sarcasm, slang, exaggeration, and expressive punctuation.\n",
    "\n",
    "For each title, VADER produces four interpretable sentiment metrics:\n",
    "\n",
    "* Negative (neg) ‚Äì proportion of the text conveying negative emotion.\n",
    "Captures anger, frustration, complaints, or pessimistic tone.\n",
    "\n",
    "* Neutral (neu) ‚Äì proportion of the text that is objective, factual, or emotionally flat.\n",
    "Neutral posts often receive lower engagement, making this a useful baseline signal.\n",
    "\n",
    "* Positive (pos) ‚Äì proportion of words carrying positive emotional tone.\n",
    "Titles expressing excitement, humor, or appreciation often perform differently in voting dynamics.\n",
    "\n",
    "* Compound (compound) ‚Äì a normalized score from ‚Äì1 to +1 summarizing overall sentiment.\n",
    "This metric balances the intensity and direction of emotion and is one of the strongest indicators of emotional valence.\n",
    "\n",
    "Adding these sentiment-derived features helps the model understand whether emotionally charged language (such as outrage, excitement, or humor) influences how many upvotes/downvotes a post receives.\n",
    "This additional signal improves prediction accuracy, especially when combined with TF‚ÄìIDF features in our Ridge regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def add_title_sentiment(df):\n",
    "    titles = df[\"title\"].astype(str)\n",
    "    scores = titles.apply(sia.polarity_scores)\n",
    "    scores_df = scores.apply(pd.Series)\n",
    "\n",
    "    df[\"title_sent_neg\"] = scores_df[\"neg\"]\n",
    "    df[\"title_sent_neu\"] = scores_df[\"neu\"]\n",
    "    df[\"title_sent_pos\"] = scores_df[\"pos\"]\n",
    "    df[\"title_sent_compound\"] = scores_df[\"compound\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "df = add_title_sentiment(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset is fully cleaned and feature-engineered, we move into the modeling phase.\n",
    "\n",
    "#### This section covers:\n",
    "\n",
    "1. Defining the feature matrix (X) and target variable (y)\n",
    "\n",
    "2. Applying log transformation to stabilize score values\n",
    "\n",
    "3. Creating train/test splits\n",
    "\n",
    "4. Building the final Ridge Regression + TF-IDF model\n",
    "\n",
    "5. Evaluating performance in both log and original score space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining X and y\n",
    "\n",
    "In this step, we select the feature columns created during feature engineering and assemble our input matrix X.\n",
    "\n",
    "We also construct the target variable y using a stabilized log transformation:\n",
    "\n",
    "y=log (score‚àímin (score)+1)\n",
    "\n",
    "This transformation reduces the influence of extremely large Reddit scores, making the model more stable and improving predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----- 1. Define X and y -----\n",
    "\n",
    "# Features\n",
    "text_features = [\"title\"]\n",
    "cat_features = [\"subreddit\"]\n",
    "num_features = [\n",
    "    \"hour\",\n",
    "    \"dayofweek\",\n",
    "    \"month\",\n",
    "    \"is_weekend\",\n",
    "    \"title_length\",\n",
    "    \"word_count\",\n",
    "    \"is_question\",\n",
    "    \"is_exclamation\",\n",
    "    \"title_sent_neg\",\n",
    "    \"title_sent_neu\",\n",
    "    \"title_sent_pos\",\n",
    "    \"title_sent_compound\",\n",
    "]\n",
    "\n",
    "feature_cols = text_features + cat_features + num_features\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "# Target: use log(1 + score) to stabilize heavy tails\n",
    "y_raw = df[\"score\"]\n",
    "min_score = y_raw.min()\n",
    "\n",
    "# shift so that the minimum becomes 1, then log-transform\n",
    "y_shifted = y_raw - min_score + 1\n",
    "y_log = np.log(y_shifted)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y_log.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature matrix and target vector are now prepared.\n",
    "Next, we split the dataset into training and testing subsets to evaluate the model on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train/Test Split\n",
    "\n",
    "We use an 80/20 split.\n",
    "A fixed random_state ensures that results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 2. Train / test split: 80 / 20 -----\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_log, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the data is fully ready for modeling.\n",
    "Next, we construct the preprocessing pipeline (TF-IDF, One-Hot Encoding, scaling) and train our final Ridge Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing + TF-IDF Vectorization\n",
    "\n",
    "This block prepares all input features:\n",
    "\n",
    "Text titles ‚Üí TF-IDF\n",
    "\n",
    "Categorical subreddit ‚Üí One-Hot Encoding\n",
    "\n",
    "Numerical features ‚Üí Passthrough\n",
    "\n",
    "This ensures that mixed data types can be fed directly into a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Models\n",
    "\n",
    "Before training more sophisticated models, we establish baselines to understand how difficult the prediction problem is.\n",
    "Baselines tell us whether our later models are actually learning anything meaningful.\n",
    "\n",
    "We compute two baselines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 1: Predict Global Mean\n",
    "\n",
    "\n",
    "The simplest possible prediction strategy is to ignore all features and always predict the global average training score.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Compute the mean of the log-transformed scores on the training set\n",
    "\n",
    "Predict this constant value for every test example\n",
    "\n",
    "Inversely transform predictions back into raw score units\n",
    "\n",
    "Compute MAE in both log space and raw score space\n",
    "\n",
    "This gives us a lower-bound benchmark: any real model must beat this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 1. Baseline in log space\n",
    "mean_pred = y_train.mean()\n",
    "y_test_pred_log = np.full_like(y_test, mean_pred, dtype=float)\n",
    "\n",
    "# 2. Inverse-transform both true and predicted values back to raw score\n",
    "y_test_raw = np.exp(y_test) + min_score - 1\n",
    "y_test_pred_raw = np.exp(y_test_pred_log) + min_score - 1\n",
    "\n",
    "# 3. MAE in original score units (upvotes minus downvotes)\n",
    "baseline_mae_raw = mean_absolute_error(y_test_raw, y_test_pred_raw)\n",
    "\n",
    "print(f\"Baseline MAE (log space): {mean_absolute_error(y_test, y_test_pred_log):.4f}\")\n",
    "print(f\"Baseline MAE (raw score): {baseline_mae_raw:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2 - Predict the Subreddit Mean (with Fallback)\n",
    "\n",
    "This baseline takes a small step beyond the global average by using subreddit-level information.\n",
    "\n",
    "The idea is straightforward:\n",
    "\n",
    "Posts from the same subreddit often get similar amounts of attention.\n",
    "So for each post in the test set, we predict its score using the average log-score of that subreddit in the training data.\n",
    "\n",
    "If a subreddit never appeared in training, we simply fall back to the overall global mean so the model can still make a prediction.\n",
    "\n",
    "Why this baseline is useful:\n",
    "\n",
    "* Different subreddits naturally have different engagement levels\n",
    "\n",
    "* The method is extremely simple to compute\n",
    "\n",
    "* It gives us a stronger comparison point than the global mean alone\n",
    "\n",
    "As with the first baseline, we report MAE in both log space and raw score space to keep our evaluation consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this baseline, we try something smarter than predicting the same score for every post.\n",
    "Instead, we assume:\n",
    "\n",
    "Posts from the same subreddit tend to receive similar levels of engagement.\n",
    "\n",
    "So for every post, we predict the average log-score of its subreddit, computed only using the training set.\n",
    "\n",
    "When the model encounters a subreddit in the test set that wasn't in training, we simply fall back to the global mean log-score.\n",
    "\n",
    "This gives us a very interpretable baseline and helps us understand how much predictive power subreddit identity carries before applying full ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Compute Subreddit Mean Log-Scores (Train Only)\n",
    "\n",
    "In this step, we calculate the average log-score for each subreddit using only the training split.\n",
    "This prevents data leakage and keeps the baseline fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ---- Baseline 2: Subreddit mean (with fallback to global mean) ----\n",
    "\n",
    "# 1. Compute mean *log-score* per subreddit on TRAIN ONLY\n",
    "train_df_for_means = pd.DataFrame(\n",
    "    {\"subreddit\": X_train[\"subreddit\"].values, \"score_log\": y_train}\n",
    ")  #  y_train is already log-transformed\n",
    "\n",
    "subreddit_means_log = train_df_for_means.groupby(\"subreddit\")[\"score_log\"].mean()\n",
    "\n",
    "# Global mean in log space (used as fallback for unseen subreddits)\n",
    "global_mean_log = y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Build a Simple Prediction Function\n",
    "\n",
    "We create a helper that:\n",
    "\n",
    "Looks up the subreddit‚Äôs mean log-score\n",
    "\n",
    "Uses the global mean if that subreddit never appeared in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_subreddit_mean_log(X, subreddit_means_log, global_mean_log):\n",
    "    \"\"\"\n",
    "    For each row in X, predict the mean *log-score* of its subreddit,\n",
    "    falling back to the global mean (log) if the subreddit was unseen in training.\n",
    "    \"\"\"\n",
    "    return X[\"subreddit\"].map(subreddit_means_log).fillna(global_mean_log).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Make Predictions on the Test Set\n",
    "\n",
    "We run the baseline predictor on the test data in log-space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Predictions on TEST set (log space)\n",
    "y_test_pred_log = predict_subreddit_mean_log(\n",
    "    X_test, subreddit_means_log, global_mean_log\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluate the Baseline (Log Space + Raw Score)\n",
    "\n",
    "Reddit scores vary hugely, so we report error:\n",
    "\n",
    "in log space (model training space)\n",
    "\n",
    "and in raw score space (actual upvotes ‚àí downvotes), which is easier to interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Metrics in LOG space\n",
    "mae_log = mean_absolute_error(y_test, y_test_pred_log)\n",
    "\n",
    "# 4. Inverse transform both true and predicted back to RAW score space\n",
    "# recall: y_log = log(score - min_score + 1)\n",
    "y_test_raw = np.exp(y_test) + min_score - 1\n",
    "y_test_pred_raw = np.exp(y_test_pred_log) + min_score - 1\n",
    "\n",
    "mae_raw = mean_absolute_error(y_test_raw, y_test_pred_raw)\n",
    "\n",
    "print(f\"  Test MAE  (log):  {mae_log:.4f}\")\n",
    "print(f\"  Test MAE  (raw):  {mae_raw:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with TF-IDF + Metadata Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with TF-IDF + Metadata Features\n",
    "\n",
    "Now that we‚Äôve established our baselines, we move on to our first real machine learning model.\n",
    "This model combines:\n",
    "\n",
    "* TF-IDF title embeddings\n",
    "\n",
    "* One-hot encoded subreddit\n",
    "\n",
    "* Numeric metadata features (time, comment count, sentiment, etc.)\n",
    "\n",
    "This gives the model a much richer representation than the baselines.\n",
    "\n",
    "We break the modeling process into 4 parts:\n",
    "\n",
    "1. Selecting feature subsets\n",
    "\n",
    "2. Building the preprocessing pipeline\n",
    "\n",
    "3. Training with cross-validation\n",
    "\n",
    "4. Evaluating on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Selecting Feature Subsets\n",
    "\n",
    "Here, we extract the text, categorical, and numeric features that were engineered earlier.\n",
    "We also create separate views of the training and test sets that match the expected input for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "RANDOM_STATE = 42  # for CV shuffling\n",
    "\n",
    "lin_feature_cols = text_features + cat_features + num_features\n",
    "\n",
    "X_train_lin = X_train[lin_feature_cols].copy()\n",
    "X_test_lin = X_test[lin_feature_cols].copy()\n",
    "\n",
    "text_col = \"title\"\n",
    "cat_cols = cat_features\n",
    "num_cols = num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Building the Preprocessing Pipeline\n",
    "\n",
    "We use a ColumnTransformer to apply three different preprocessing steps at once:\n",
    "\n",
    "TF-IDF for the post title\n",
    "\n",
    "One-hot encoding for subreddit\n",
    "\n",
    "Standard scaling for numeric features\n",
    "\n",
    "All of this is wrapped in a scikit-learn pipeline so preprocessing happens automatically during training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_lin = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"text\",\n",
    "            TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=5),\n",
    "            text_col,\n",
    "        ),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "            cat_cols,\n",
    "        ),\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            num_cols,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "lin_model = LinearRegression()\n",
    "\n",
    "lin_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor_lin),\n",
    "        (\"model\", lin_model),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-Fold Cross-Validation on the Training Set\n",
    "\n",
    "#### 3. Cross-Validation (5-Fold)\n",
    "\n",
    "To check that our model generalizes well, we run 5-fold cross-validation on the training set.\n",
    "This gives us a more reliable estimate of performance than a single train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 fold CV for consistency\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    lin_pipeline,\n",
    "    X_train_lin,\n",
    "    y_train,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final training + test evaluation\n",
    "\n",
    "4. Final Training and Evaluation\n",
    "\n",
    "We now train the model on the full training set and evaluate on the test set.\n",
    "\n",
    "As before, we report MAE in both:\n",
    "\n",
    "* Log space (model's actual prediction space)\n",
    "\n",
    "* Raw score space (original Reddit score units, easier to interpret)\n",
    "\n",
    "We invert the log-transform using the same shifting we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_pipeline.fit(X_train_lin, y_train)\n",
    "\n",
    "# Predictions in log space\n",
    "y_test_pred_log_lin = lin_pipeline.predict(X_test_lin)\n",
    "\n",
    "# Log-space metrics\n",
    "lin_mae_log = mean_absolute_error(y_test, y_test_pred_log_lin)\n",
    "\n",
    "# Inverse-transform back to raw score space\n",
    "y_test_raw = np.exp(y_test) + min_score - 1\n",
    "y_test_pred_raw_lin = np.exp(y_test_pred_log_lin) + min_score - 1\n",
    "\n",
    "lin_mae_raw = mean_absolute_error(y_test_raw, y_test_pred_raw_lin)\n",
    "\n",
    "print(f\"  Test MAE  (log): {lin_mae_log:.4f}\")\n",
    "print(f\"  Test MAE  (raw): {lin_mae_raw:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Champion Model: Ridge Regression with TF-IDF + Metadata Features\n",
    "\n",
    "After testing multiple approaches, Ridge Regression turned out to be the strongest model.\n",
    "It performs extremely well with high-dimensional TF-IDF text features and avoids overfitting through L2 regularization.\n",
    "\n",
    "This model combines:\n",
    "\n",
    "* TF-IDF representations of post titles\n",
    "\n",
    "* One-hot encoded subreddit\n",
    "\n",
    "* Scaled numeric metadata features\n",
    "\n",
    "* Ridge Regression, tuned via cross-validation\n",
    "\n",
    "We break the setup into 4 steps:\n",
    "\n",
    "1. Preparing the training and test feature matrices\n",
    "\n",
    "2. Building the preprocessing pipeline\n",
    "\n",
    "3. Running a hyperparameter search (alpha)\n",
    "\n",
    "4. Evaluating the final model (log space + raw score space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing Training and Test Data\n",
    "\n",
    "We start by selecting the text, categorical, and numeric feature groups.\n",
    "These were all created earlier during feature engineering.\n",
    "\n",
    "We then define X_train_ridge and X_test_ridge to match the exact input expected by our Ridge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "ridge_feature_cols = text_features + cat_features + num_features\n",
    "\n",
    "X_train_ridge = X_train[ridge_feature_cols].copy()\n",
    "X_test_ridge = X_test[ridge_feature_cols].copy()\n",
    "\n",
    "\n",
    "text_col = \"title\"\n",
    "cat_cols = cat_features\n",
    "num_cols = num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Build the Preprocessing Pipeline\n",
    "\n",
    "\n",
    "2. Building the Preprocessing Pipeline\n",
    "\n",
    "Ridge Regression must receive numerical inputs, so we use a ColumnTransformer to process each feature type:\n",
    "\n",
    "* TF-IDF for the title\n",
    "\n",
    "* One-hot encoding for subreddit\n",
    "\n",
    "* Scaling for numeric metadata\n",
    "\n",
    "This ensures consistent preprocessing during both training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"text\",\n",
    "            TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=5),\n",
    "            text_col,\n",
    "        ),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "            cat_cols,\n",
    "        ),\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            num_cols,\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Set Up Ridge + Grid Search\n",
    "\n",
    "3. Hyperparameter Tuning (Grid Search)\n",
    "\n",
    "We wrap Ridge Regression in a scikit-learn Pipeline and tune its key parameter alpha\n",
    "using 5-fold cross-validation.\n",
    "\n",
    "alpha controls how strong the L2 penalty is ‚Äî higher = more regularization.\n",
    "\n",
    "The model is evaluated using MAE in log-space, the same space we trained in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 fold CV\n",
    "base_ridge = Ridge(random_state=RANDOM_STATE)\n",
    "\n",
    "ridge_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", base_ridge),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = {\"model__alpha\": [0.01, 0.1, 1, 3, 10, 30, 100]}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=ridge_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_absolute_error\",  #!!!! MAE in LOG space\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# y_train is log-transformed already!!!!! dont forget\n",
    "grid.fit(X_train_ridge, y_train)\n",
    "\n",
    "\n",
    "best_ridge = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Evaluate the Champion Ridge Model\n",
    "\n",
    "#### 4. Final Evaluation (Log Space + Raw Score Space)\n",
    "\n",
    "We evaluate the best Ridge model from the grid search.\n",
    "\n",
    "We compute:\n",
    "\n",
    "* MAE in log space ‚Üí model‚Äôs training space\n",
    "\n",
    "* MAE in raw space ‚Üí the actual Reddit score units\n",
    "\n",
    "Raw MAE is the most interpretable metric, so we treat that as our final benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict in log(shifted score) space\n",
    "y_test_pred_log = best_ridge.predict(X_test_ridge)\n",
    "# Metrics in LOG space\n",
    "mae_log = mean_absolute_error(y_test, y_test_pred_log)\n",
    "\n",
    "y_test_pred_log = best_ridge.predict(X_test_ridge)\n",
    "# log metrics\n",
    "mae_log = mean_absolute_error(y_test, y_test_pred_log)\n",
    "# back to raw\n",
    "y_test_raw = np.exp(y_test) + min_score - 1\n",
    "y_test_pred_raw = np.exp(y_test_pred_log) + min_score - 1\n",
    "mae_raw = mean_absolute_error(y_test_raw, y_test_pred_raw)\n",
    "print(\"Ridge MAE (log):\", mae_log)\n",
    "print(\"Ridge MAE (raw):\", mae_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why I Chose Ridge Regression as the Final Model\n",
    "\n",
    "After experimenting with multiple modeling approaches for predicting Reddit post scores, I ultimately selected Ridge Regression as my final model. This decision wasn‚Äôt just based on theoretical expectations ‚Äî it was strongly supported by the structure of my dataset and the performance patterns I saw in my experiments.\n",
    "\n",
    "#### 1. Ridge fits the structure of my TF-IDF + metadata feature space\n",
    "\n",
    "Once I applied TF-IDF to the post titles (with up to 20,000 features and bi-grams), my dataset became extremely high-dimensional and sparse. This is exactly the type of setting where Ridge Regression is known to work well.\n",
    "\n",
    "Here‚Äôs what I found when comparing it to other models:\n",
    "\n",
    "* Tree-based models like Random Forest and Gradient Boosting performed very poorly.\n",
    "In my tests, they produced MAE values in the hundreds, meaning they failed to generalize on sparse, high-dimensional inputs.\n",
    "\n",
    "* Ordinary Linear Regression handled TF-IDF better than the trees, but it still showed clear signs of overfitting, landing around 42‚Äì43 MAE in raw score space.\n",
    "\n",
    "Ridge Regression, on the other hand, immediately introduced stability.\n",
    "Because it uses L2 regularization, it prevents the massive coefficient swings that tend to happen with thousands of correlated TF-IDF features. This behavior perfectly matches both the theory behind Ridge and the improvements I saw in practice.\n",
    "\n",
    "#### 2. Ridge Regression clearly outperformed every other model I tested\n",
    "\n",
    "The results made the decision very straightforward.\n",
    "\n",
    "Across all of the models I tried ‚Äî including:\n",
    "\n",
    "* Global-mean and subreddit-mean baselines\n",
    "\n",
    "* Linear Regression with TF-IDF\n",
    "\n",
    "* Random Forest\n",
    "\n",
    "* Gradient Boosting Regressor\n",
    "\n",
    "* HistGradientBoosting\n",
    "\n",
    "Stacking models\n",
    "\n",
    "Ridge Regression with hyperparameter tuning\n",
    "\n",
    "Ridge Regression consistently delivered the lowest MAE.\n",
    "\n",
    "My best Ridge model achieved:\n",
    "\n",
    "#### FINAL: 40.03 MAE in raw score space\n",
    "\n",
    "This was a dramatic improvement:\n",
    "\n",
    "#### Baselines: ~60‚Äì70 MAE\n",
    "\n",
    "#### TF-IDF Linear Regression: ~42 MAE\n",
    "\n",
    "#### Tree-based models: 100+ MAE\n",
    "\n",
    "#### Stackers: Couldn‚Äôt outperform Ridge and often struggled with NaNs\n",
    "\n",
    "The fact that Ridge cut the MAE almost in half compared to ordinary Linear Regression highlighted how important regularization was for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
