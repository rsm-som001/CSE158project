{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "def readCSV(path):\n",
    "  f = gzip.open(path, 'rt')\n",
    "  f.readline()\n",
    "  for l in f:\n",
    "    yield l.strip().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#image_id</th>\n",
       "      <th>unixtime</th>\n",
       "      <th>rawtime</th>\n",
       "      <th>title</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>number_of_upvotes</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>number_of_downvotes</th>\n",
       "      <th>localtime</th>\n",
       "      <th>score</th>\n",
       "      <th>number_of_comments</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333172e+09</td>\n",
       "      <td>2012-03-31T12:40:39.590113-07:00</td>\n",
       "      <td>And here's a downvote.</td>\n",
       "      <td>63470.0</td>\n",
       "      <td>rmqjs</td>\n",
       "      <td>32657.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>30813.0</td>\n",
       "      <td>1.333198e+09</td>\n",
       "      <td>1844.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>Animates_Everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333178e+09</td>\n",
       "      <td>2012-03-31T14:16:01.093638-07:00</td>\n",
       "      <td>Expectation</td>\n",
       "      <td>35.0</td>\n",
       "      <td>rmun4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.333203e+09</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Gangsta_Raper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333200e+09</td>\n",
       "      <td>2012-03-31T20:18:33.192906-07:00</td>\n",
       "      <td>Downvote</td>\n",
       "      <td>41.0</td>\n",
       "      <td>rna86</td>\n",
       "      <td>32.0</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.333225e+09</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gangsta_Raper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333252e+09</td>\n",
       "      <td>2012-04-01T10:52:10-07:00</td>\n",
       "      <td>Every time I downvote something</td>\n",
       "      <td>10.0</td>\n",
       "      <td>ro7e4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.333278e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gangsta_Raper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333273e+09</td>\n",
       "      <td>2012-04-01T16:35:54.393381-07:00</td>\n",
       "      <td>Downvote &amp;quot;Dies Irae&amp;quot;</td>\n",
       "      <td>65.0</td>\n",
       "      <td>rooof</td>\n",
       "      <td>57.0</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.333298e+09</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gangsta_Raper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132298</th>\n",
       "      <td>9998</td>\n",
       "      <td>1.344760e+09</td>\n",
       "      <td>2012-08-12T15:24:06-07:00</td>\n",
       "      <td>OM NOM NOM</td>\n",
       "      <td>34.0</td>\n",
       "      <td>y41wv</td>\n",
       "      <td>25.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.344785e+09</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vaggietales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132299</th>\n",
       "      <td>9998</td>\n",
       "      <td>1.345270e+09</td>\n",
       "      <td>2012-08-18T13:09:38-07:00</td>\n",
       "      <td>Don't feed the animals...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>yfw66</td>\n",
       "      <td>14.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.345295e+09</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Deydria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132300</th>\n",
       "      <td>9998</td>\n",
       "      <td>1.345954e+09</td>\n",
       "      <td>2012-08-26T04:06:02+00:00</td>\n",
       "      <td>WTF worthy.</td>\n",
       "      <td>49.0</td>\n",
       "      <td>yu838</td>\n",
       "      <td>26.0</td>\n",
       "      <td>WTF</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.345954e+09</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>beatlesrock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132301</th>\n",
       "      <td>9998</td>\n",
       "      <td>1.346626e+09</td>\n",
       "      <td>2012-09-02T22:45:06+00:00</td>\n",
       "      <td>Just a camel eating a kids head, welcome to th...</td>\n",
       "      <td>123.0</td>\n",
       "      <td>z91ah</td>\n",
       "      <td>65.0</td>\n",
       "      <td>WTF</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.346626e+09</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>v7o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132302</th>\n",
       "      <td>9998</td>\n",
       "      <td>1.347718e+09</td>\n",
       "      <td>2012-09-15T14:00:43+00:00</td>\n",
       "      <td>Ass is looking at a little girl eaten by camel.</td>\n",
       "      <td>425.0</td>\n",
       "      <td>zxbsj</td>\n",
       "      <td>272.0</td>\n",
       "      <td>WTF</td>\n",
       "      <td>153.0</td>\n",
       "      <td>1.347718e+09</td>\n",
       "      <td>119.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NSFW_PORN_ONLY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132303 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        #image_id      unixtime                           rawtime  \\\n",
       "0               0  1.333172e+09  2012-03-31T12:40:39.590113-07:00   \n",
       "1               0  1.333178e+09  2012-03-31T14:16:01.093638-07:00   \n",
       "2               0  1.333200e+09  2012-03-31T20:18:33.192906-07:00   \n",
       "3               0  1.333252e+09         2012-04-01T10:52:10-07:00   \n",
       "4               0  1.333273e+09  2012-04-01T16:35:54.393381-07:00   \n",
       "...           ...           ...                               ...   \n",
       "132298       9998  1.344760e+09         2012-08-12T15:24:06-07:00   \n",
       "132299       9998  1.345270e+09         2012-08-18T13:09:38-07:00   \n",
       "132300       9998  1.345954e+09         2012-08-26T04:06:02+00:00   \n",
       "132301       9998  1.346626e+09         2012-09-02T22:45:06+00:00   \n",
       "132302       9998  1.347718e+09         2012-09-15T14:00:43+00:00   \n",
       "\n",
       "                                                    title  total_votes  \\\n",
       "0                                  And here's a downvote.      63470.0   \n",
       "1                                             Expectation         35.0   \n",
       "2                                                Downvote         41.0   \n",
       "3                         Every time I downvote something         10.0   \n",
       "4                          Downvote &quot;Dies Irae&quot;         65.0   \n",
       "...                                                   ...          ...   \n",
       "132298                                         OM NOM NOM         34.0   \n",
       "132299                          Don't feed the animals...         19.0   \n",
       "132300                                        WTF worthy.         49.0   \n",
       "132301  Just a camel eating a kids head, welcome to th...        123.0   \n",
       "132302    Ass is looking at a little girl eaten by camel.        425.0   \n",
       "\n",
       "       reddit_id  number_of_upvotes subreddit  number_of_downvotes  \\\n",
       "0          rmqjs            32657.0     funny              30813.0   \n",
       "1          rmun4               29.0  GifSound                  6.0   \n",
       "2          rna86               32.0  GifSound                  9.0   \n",
       "3          ro7e4                6.0  GifSound                  4.0   \n",
       "4          rooof               57.0  GifSound                  8.0   \n",
       "...          ...                ...       ...                  ...   \n",
       "132298     y41wv               25.0     funny                  9.0   \n",
       "132299     yfw66               14.0     funny                  5.0   \n",
       "132300     yu838               26.0       WTF                 23.0   \n",
       "132301     z91ah               65.0       WTF                 58.0   \n",
       "132302     zxbsj              272.0       WTF                153.0   \n",
       "\n",
       "           localtime   score  number_of_comments             username  \n",
       "0       1.333198e+09  1844.0               622.0  Animates_Everything  \n",
       "1       1.333203e+09    23.0                 3.0        Gangsta_Raper  \n",
       "2       1.333225e+09    23.0                 0.0        Gangsta_Raper  \n",
       "3       1.333278e+09     2.0                 0.0        Gangsta_Raper  \n",
       "4       1.333298e+09    49.0                 0.0        Gangsta_Raper  \n",
       "...              ...     ...                 ...                  ...  \n",
       "132298  1.344785e+09    16.0                 0.0          vaggietales  \n",
       "132299  1.345295e+09     9.0                 2.0              Deydria  \n",
       "132300  1.345954e+09     3.0                 6.0          beatlesrock  \n",
       "132301  1.346626e+09     7.0                12.0                  v7o  \n",
       "132302  1.347718e+09   119.0                10.0       NSFW_PORN_ONLY  \n",
       "\n",
       "[132303 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/Users/andrewchen/CSE158/Assignment_2/redditSubmissions.csv.gz\",\n",
    "    compression=\"gzip\",\n",
    "    on_bad_lines=\"skip\",\n",
    "    engine=\"python\"\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#image_id</th>\n",
       "      <th>unixtime</th>\n",
       "      <th>rawtime</th>\n",
       "      <th>title</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>number_of_upvotes</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>number_of_downvotes</th>\n",
       "      <th>localtime</th>\n",
       "      <th>score</th>\n",
       "      <th>number_of_comments</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333761e+09</td>\n",
       "      <td>2012-04-07T08:11:00-07:00</td>\n",
       "      <td>Demolished, every time you downvote someone</td>\n",
       "      <td>40.0</td>\n",
       "      <td>rxwjg</td>\n",
       "      <td>17.0</td>\n",
       "      <td>gifs</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.333786e+09</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Hellothereawesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1.339160e+09</td>\n",
       "      <td>2012-06-08T19:54:35.421944-07:00</td>\n",
       "      <td>getting that first downvote on a new post</td>\n",
       "      <td>13.0</td>\n",
       "      <td>usmxn</td>\n",
       "      <td>5.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.339185e+09</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1.339408e+09</td>\n",
       "      <td>2012-06-11T16:44:39.947798-07:00</td>\n",
       "      <td>How reddit seems to reacts whenever I share a ...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>uwzrd</td>\n",
       "      <td>6.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.339433e+09</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1.339425e+09</td>\n",
       "      <td>2012-06-11T21:34:51.692933-07:00</td>\n",
       "      <td>Every LastAirBender post with a NSFW tag</td>\n",
       "      <td>20.0</td>\n",
       "      <td>uxf5q</td>\n",
       "      <td>9.0</td>\n",
       "      <td>pics</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.339450e+09</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HadManySons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1.340008e+09</td>\n",
       "      <td>2012-06-18T15:28:35.800140-07:00</td>\n",
       "      <td>How I felt when i forgot to put &amp;quot;spoiler&amp;...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>v8vl7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>gifs</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.340033e+09</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TraumaticASH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    #image_id      unixtime                           rawtime  \\\n",
       "5           0  1.333761e+09         2012-04-07T08:11:00-07:00   \n",
       "7           0  1.339160e+09  2012-06-08T19:54:35.421944-07:00   \n",
       "8           0  1.339408e+09  2012-06-11T16:44:39.947798-07:00   \n",
       "9           0  1.339425e+09  2012-06-11T21:34:51.692933-07:00   \n",
       "10          0  1.340008e+09  2012-06-18T15:28:35.800140-07:00   \n",
       "\n",
       "                                                title  total_votes reddit_id  \\\n",
       "5         Demolished, every time you downvote someone         40.0     rxwjg   \n",
       "7           getting that first downvote on a new post         13.0     usmxn   \n",
       "8   How reddit seems to reacts whenever I share a ...         14.0     uwzrd   \n",
       "9            Every LastAirBender post with a NSFW tag         20.0     uxf5q   \n",
       "10  How I felt when i forgot to put &quot;spoiler&...         21.0     v8vl7   \n",
       "\n",
       "    number_of_upvotes subreddit  number_of_downvotes     localtime  score  \\\n",
       "5                17.0      gifs                 23.0  1.333786e+09   -6.0   \n",
       "7                 5.0     funny                  8.0  1.339185e+09   -3.0   \n",
       "8                 6.0     funny                  8.0  1.339433e+09   -2.0   \n",
       "9                 9.0      pics                 11.0  1.339450e+09   -2.0   \n",
       "10               10.0      gifs                 11.0  1.340033e+09   -1.0   \n",
       "\n",
       "    number_of_comments           username  \n",
       "5                  3.0  Hellothereawesome  \n",
       "7                  0.0                NaN  \n",
       "8                  0.0                NaN  \n",
       "9                  0.0        HadManySons  \n",
       "10                 0.0       TraumaticASH  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 21k rows of negative scores\n",
    "df[df['score'] < 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Validation\n",
    "\n",
    "Before building any models, we first validate that the core numeric fields in the dataset are internally consistent and that there are no obvious data quality issues.\n",
    "\n",
    "**Checks performed:**\n",
    "\n",
    "1. **Score consistency**\n",
    "   We verify that the Reddit `score` field matches the definition:\n",
    "      `score` = `number_of_upvotes` - `number_of_downvotes`\n",
    "\n",
    "2. **Non-negative votes**\n",
    "\n",
    "   We confirm that `number_of_upvotes` and `number_of_downvotes` are never negative.  \n",
    "\n",
    "3. **Duplicated posts**\n",
    "\n",
    "   Using `reddit_id` as the unique identifier of a submission, we check for duplicated `reddit_id`s.  \n",
    "\n",
    "4. **Missing values**\n",
    "\n",
    "   Finally, we compute the total number of missing values in the dataset\n",
    "      - The `username` column has 20,260 missing values, but will not be considered in our model\n",
    "      - Every other column only has 1 missing value on the same row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score consistency check (should be 0): 0.0\n",
      "Number of rows with negative votes (should be 0): 0\n",
      "Number of duplicated reddit_id values: 93\n",
      "#image_id                  0\n",
      "unixtime                   1\n",
      "rawtime                    1\n",
      "title                      1\n",
      "total_votes                1\n",
      "reddit_id                  1\n",
      "number_of_upvotes          1\n",
      "subreddit                  1\n",
      "number_of_downvotes        1\n",
      "localtime                  1\n",
      "score                      1\n",
      "number_of_comments         1\n",
      "username               20260\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Score = upvotes - downvotes\n",
    "score_diff = (df['number_of_upvotes'] - df['number_of_downvotes'] - df['score']).abs().sum()\n",
    "print(\"Score consistency check (should be 0):\", score_diff)\n",
    "\n",
    "# Non-negative votes\n",
    "\n",
    "neg_votes = ((df['number_of_upvotes'] < 0) | (df['number_of_downvotes'] < 0)).sum()\n",
    "print(\"Number of rows with negative votes (should be 0):\", neg_votes)\n",
    "\n",
    "dup_count = df['reddit_id'].duplicated().sum()\n",
    "print(\"Number of duplicated reddit_id values:\", dup_count)\n",
    "\n",
    "# Missing Values\n",
    "df_nan = df.isna().sum()\n",
    "print(df_nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Type Casting and Data Cleaning\n",
    "\n",
    "After validating the core relationships between the voting fields, we standardize column types and handle missing or duplicated data.\n",
    "\n",
    "**2.1 Type casting**\n",
    "\n",
    "To avoid subtle bugs during modeling, we explicitly cast numeric columns to appropriate types:\n",
    "\n",
    "- `unixtime` → numeric (for conversion to timestamps)\n",
    "- `total_votes`, `number_of_upvotes`, `number_of_downvotes`, `score` → `float`\n",
    "\n",
    "\n",
    "**2.2 Handling duplicates**\n",
    "\n",
    "We treat each `reddit_id` as a unique Reddit submission. If the same `reddit_id` appears multiple times, it is most likely a duplication introduced during data collection \n",
    "- Drop duplicate rows based on `reddit_id`, keeping a single canonical copy of each post.\n",
    "\n",
    "**2.3 Handling missing values**\n",
    "\n",
    "After type casting, we re-check for missing values.\n",
    "\n",
    "- Since `username` has 20,260 values, we will drop the column now\n",
    "- Because there is only a very small number of rows with missing data, we drop rows containing any `NaN` via `dropna()`.\n",
    "\n",
    "**2.4 Title Text Cleaning**\n",
    "\n",
    "The raw `title` field contains HTML entities (e.g., `&quot;`) and inconsistent whitespace from the SNAP export. Since titles are used for TF-IDF features and sentiment analysis, we apply light normalization to improve text quality while preserving meaning.\n",
    "\n",
    "Cleaning steps:\n",
    "- Convert to string to avoid type issues  \n",
    "- Unescape HTML entities (e.g., `&quot;` → `\"`)  \n",
    "- Strip leading/trailing whitespace  \n",
    "- Collapse repeated spaces/newlines into a single space  \n",
    "- Lowercase for consistent text modeling  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#image_id</th>\n",
       "      <th>unixtime</th>\n",
       "      <th>rawtime</th>\n",
       "      <th>title</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>number_of_upvotes</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>number_of_downvotes</th>\n",
       "      <th>localtime</th>\n",
       "      <th>score</th>\n",
       "      <th>number_of_comments</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333172e+09</td>\n",
       "      <td>2012-03-31T12:40:39.590113-07:00</td>\n",
       "      <td>And here's a downvote.</td>\n",
       "      <td>63470.0</td>\n",
       "      <td>rmqjs</td>\n",
       "      <td>32657.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>30813.0</td>\n",
       "      <td>1.333198e+09</td>\n",
       "      <td>1844.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>Animates_Everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333178e+09</td>\n",
       "      <td>2012-03-31T14:16:01.093638-07:00</td>\n",
       "      <td>Expectation</td>\n",
       "      <td>35.0</td>\n",
       "      <td>rmun4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.333203e+09</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Gangsta_Raper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333200e+09</td>\n",
       "      <td>2012-03-31T20:18:33.192906-07:00</td>\n",
       "      <td>Downvote</td>\n",
       "      <td>41.0</td>\n",
       "      <td>rna86</td>\n",
       "      <td>32.0</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.333225e+09</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gangsta_Raper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333252e+09</td>\n",
       "      <td>2012-04-01T10:52:10-07:00</td>\n",
       "      <td>Every time I downvote something</td>\n",
       "      <td>10.0</td>\n",
       "      <td>ro7e4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.333278e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gangsta_Raper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333273e+09</td>\n",
       "      <td>2012-04-01T16:35:54.393381-07:00</td>\n",
       "      <td>Downvote &amp;quot;Dies Irae&amp;quot;</td>\n",
       "      <td>65.0</td>\n",
       "      <td>rooof</td>\n",
       "      <td>57.0</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.333298e+09</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gangsta_Raper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132298</th>\n",
       "      <td>9998</td>\n",
       "      <td>1.344760e+09</td>\n",
       "      <td>2012-08-12T15:24:06-07:00</td>\n",
       "      <td>OM NOM NOM</td>\n",
       "      <td>34.0</td>\n",
       "      <td>y41wv</td>\n",
       "      <td>25.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.344785e+09</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vaggietales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132299</th>\n",
       "      <td>9998</td>\n",
       "      <td>1.345270e+09</td>\n",
       "      <td>2012-08-18T13:09:38-07:00</td>\n",
       "      <td>Don't feed the animals...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>yfw66</td>\n",
       "      <td>14.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.345295e+09</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Deydria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132300</th>\n",
       "      <td>9998</td>\n",
       "      <td>1.345954e+09</td>\n",
       "      <td>2012-08-26T04:06:02+00:00</td>\n",
       "      <td>WTF worthy.</td>\n",
       "      <td>49.0</td>\n",
       "      <td>yu838</td>\n",
       "      <td>26.0</td>\n",
       "      <td>WTF</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.345954e+09</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>beatlesrock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132301</th>\n",
       "      <td>9998</td>\n",
       "      <td>1.346626e+09</td>\n",
       "      <td>2012-09-02T22:45:06+00:00</td>\n",
       "      <td>Just a camel eating a kids head, welcome to th...</td>\n",
       "      <td>123.0</td>\n",
       "      <td>z91ah</td>\n",
       "      <td>65.0</td>\n",
       "      <td>WTF</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.346626e+09</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>v7o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132302</th>\n",
       "      <td>9998</td>\n",
       "      <td>1.347718e+09</td>\n",
       "      <td>2012-09-15T14:00:43+00:00</td>\n",
       "      <td>Ass is looking at a little girl eaten by camel.</td>\n",
       "      <td>425.0</td>\n",
       "      <td>zxbsj</td>\n",
       "      <td>272.0</td>\n",
       "      <td>WTF</td>\n",
       "      <td>153.0</td>\n",
       "      <td>1.347718e+09</td>\n",
       "      <td>119.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NSFW_PORN_ONLY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132303 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        #image_id      unixtime                           rawtime  \\\n",
       "0               0  1.333172e+09  2012-03-31T12:40:39.590113-07:00   \n",
       "1               0  1.333178e+09  2012-03-31T14:16:01.093638-07:00   \n",
       "2               0  1.333200e+09  2012-03-31T20:18:33.192906-07:00   \n",
       "3               0  1.333252e+09         2012-04-01T10:52:10-07:00   \n",
       "4               0  1.333273e+09  2012-04-01T16:35:54.393381-07:00   \n",
       "...           ...           ...                               ...   \n",
       "132298       9998  1.344760e+09         2012-08-12T15:24:06-07:00   \n",
       "132299       9998  1.345270e+09         2012-08-18T13:09:38-07:00   \n",
       "132300       9998  1.345954e+09         2012-08-26T04:06:02+00:00   \n",
       "132301       9998  1.346626e+09         2012-09-02T22:45:06+00:00   \n",
       "132302       9998  1.347718e+09         2012-09-15T14:00:43+00:00   \n",
       "\n",
       "                                                    title  total_votes  \\\n",
       "0                                  And here's a downvote.      63470.0   \n",
       "1                                             Expectation         35.0   \n",
       "2                                                Downvote         41.0   \n",
       "3                         Every time I downvote something         10.0   \n",
       "4                          Downvote &quot;Dies Irae&quot;         65.0   \n",
       "...                                                   ...          ...   \n",
       "132298                                         OM NOM NOM         34.0   \n",
       "132299                          Don't feed the animals...         19.0   \n",
       "132300                                        WTF worthy.         49.0   \n",
       "132301  Just a camel eating a kids head, welcome to th...        123.0   \n",
       "132302    Ass is looking at a little girl eaten by camel.        425.0   \n",
       "\n",
       "       reddit_id  number_of_upvotes subreddit  number_of_downvotes  \\\n",
       "0          rmqjs            32657.0     funny              30813.0   \n",
       "1          rmun4               29.0  GifSound                  6.0   \n",
       "2          rna86               32.0  GifSound                  9.0   \n",
       "3          ro7e4                6.0  GifSound                  4.0   \n",
       "4          rooof               57.0  GifSound                  8.0   \n",
       "...          ...                ...       ...                  ...   \n",
       "132298     y41wv               25.0     funny                  9.0   \n",
       "132299     yfw66               14.0     funny                  5.0   \n",
       "132300     yu838               26.0       WTF                 23.0   \n",
       "132301     z91ah               65.0       WTF                 58.0   \n",
       "132302     zxbsj              272.0       WTF                153.0   \n",
       "\n",
       "           localtime   score  number_of_comments             username  \n",
       "0       1.333198e+09  1844.0               622.0  Animates_Everything  \n",
       "1       1.333203e+09    23.0                 3.0        Gangsta_Raper  \n",
       "2       1.333225e+09    23.0                 0.0        Gangsta_Raper  \n",
       "3       1.333278e+09     2.0                 0.0        Gangsta_Raper  \n",
       "4       1.333298e+09    49.0                 0.0        Gangsta_Raper  \n",
       "...              ...     ...                 ...                  ...  \n",
       "132298  1.344785e+09    16.0                 0.0          vaggietales  \n",
       "132299  1.345295e+09     9.0                 2.0              Deydria  \n",
       "132300  1.345954e+09     3.0                 6.0          beatlesrock  \n",
       "132301  1.346626e+09     7.0                12.0                  v7o  \n",
       "132302  1.347718e+09   119.0                10.0       NSFW_PORN_ONLY  \n",
       "\n",
       "[132303 rows x 13 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unixtime'] = pd.to_numeric(df['unixtime'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#image_id                  0\n",
       "unixtime                   1\n",
       "rawtime                    1\n",
       "title                      1\n",
       "total_votes                1\n",
       "reddit_id                  1\n",
       "number_of_upvotes          1\n",
       "subreddit                  1\n",
       "number_of_downvotes        1\n",
       "localtime                  1\n",
       "score                      1\n",
       "number_of_comments         1\n",
       "username               20260\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated reddit_id values: 0\n",
      "#image_id              0\n",
      "unixtime               0\n",
      "rawtime                0\n",
      "title                  0\n",
      "total_votes            0\n",
      "reddit_id              0\n",
      "number_of_upvotes      0\n",
      "subreddit              0\n",
      "number_of_downvotes    0\n",
      "localtime              0\n",
      "score                  0\n",
      "number_of_comments     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Casting \n",
    "df['unixtime'] = pd.to_numeric(df['unixtime'], errors='coerce')\n",
    "df['total_votes'] = df['total_votes'].astype(float)\n",
    "df['number_of_upvotes'] = df['number_of_upvotes'].astype(float)\n",
    "df['number_of_downvotes'] = df['number_of_downvotes'].astype(float)\n",
    "df['score'] = df['score'].astype(float)\n",
    "\n",
    "# Duplicate Handling \n",
    "df = df.drop_duplicates(subset=['reddit_id'])\n",
    "dup_reddit_ids = df['reddit_id'].duplicated().sum()\n",
    "print(\"Number of duplicated reddit_id values:\", dup_reddit_ids)\n",
    "\n",
    "# Missing Values Handling\n",
    "df = df.drop(columns=['username'])\n",
    "df = df.dropna()\n",
    "print(df.isna().sum())\n",
    "\n",
    "\n",
    "def clean_title(text):\n",
    "    # 1. Ensure string\n",
    "    s = str(text)\n",
    "    # 2. Unescape HTML entities: &quot; &amp; &lt; &gt; etc.\n",
    "    s = html.unescape(s)\n",
    "    # 3. Strip leading/trailing whitespace\n",
    "    s = s.strip()\n",
    "    # 4. Collapse multiple spaces/newlines into a single space\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    # 5. (Optional) lowercase for modeling\n",
    "    s = s.lower()\n",
    "    return s\n",
    "df['title_clean'] = df['title'].apply(clean_title)\n",
    "df['title'] = df['title_clean']\n",
    "df.drop(columns=['title_clean'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "With a clean base dataset, we create additional features that capture temporal patterns and simple properties of the post titles. These engineered features will be used as inputs to our predictive models.\n",
    "\n",
    "**3.1 Time-based features**\n",
    "\n",
    "The dataset provides `unixtime`, which is the submission time in seconds since the Unix epoch. We convert this into a Python `datetime` object and then derive several interpretable time features:\n",
    "\n",
    "- `datetime`: full timestamp converted from `unixtime`\n",
    "- `hour`: hour of day the post was submitted (0–23)\n",
    "- `dayofweek`: day of week (0 = Monday, …, 6 = Sunday)\n",
    "- `year`: calendar year\n",
    "\n",
    "**3.2 Title-based features**\n",
    "\n",
    "We also construct simple structural features from the post title:\n",
    "\n",
    "- `title_length`: number of characters in the title\n",
    "- `word_count`: number of whitespace-separated tokens in the title\n",
    "\n",
    "**3.3 Sentiment-based features **\n",
    "\n",
    "We augment these with sentiment scores derived from the cleaned title text (negative, neutral, positive, and compound sentiment), but the core structural features are already set up here.\n",
    "\n",
    "Together, the time-based and title-based features give our models additional signal beyond raw vote counts and subreddit identity, while remaining interpretable and easy to reason about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dtypes:\n",
      "#image_id                       int64\n",
      "unixtime                      float64\n",
      "rawtime                        object\n",
      "title                          object\n",
      "total_votes                   float64\n",
      "reddit_id                      object\n",
      "number_of_upvotes             float64\n",
      "subreddit                      object\n",
      "number_of_downvotes           float64\n",
      "localtime                     float64\n",
      "score                         float64\n",
      "number_of_comments            float64\n",
      "datetime               datetime64[ns]\n",
      "hour                            int32\n",
      "dayofweek                       int32\n",
      "year                            int32\n",
      "title_length                    int64\n",
      "word_count                      int64\n",
      "title_sent_neg                float64\n",
      "title_sent_neu                float64\n",
      "title_sent_pos                float64\n",
      "title_sent_compound           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['datetime'] = pd.to_datetime(df['unixtime'], unit='s')\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['title'] = df['title'].astype(str).str.strip()\n",
    "df['title_length'] = df['title'].str.len()\n",
    "df['word_count'] = df['title'].str.split().apply(len)\n",
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def add_title_sentiment(df):\n",
    "    titles = df['title'].astype(str)\n",
    "\n",
    "    scores = titles.apply(sia.polarity_scores) \n",
    "    scores_df = scores.apply(pd.Series)\n",
    "\n",
    "    df['title_sent_neg'] = scores_df['neg']\n",
    "    df['title_sent_neu'] = scores_df['neu']\n",
    "    df['title_sent_pos'] = scores_df['pos']\n",
    "    df['title_sent_compound'] = scores_df['compound']\n",
    "    return df\n",
    "# add new column\n",
    "df = add_title_sentiment(df)\n",
    "\n",
    "\n",
    "print(\"\\nFinal dtypes:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['#image_id', 'unixtime', 'rawtime', 'title', 'total_votes', 'reddit_id',\n",
       "       'number_of_upvotes', 'subreddit', 'number_of_downvotes', 'localtime',\n",
       "       'score', 'number_of_comments', 'datetime', 'hour', 'dayofweek', 'year',\n",
       "       'title_length', 'word_count', 'title_sent_neg', 'title_sent_neu',\n",
       "       'title_sent_pos', 'title_sent_compound'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixtime</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>number_of_upvotes</th>\n",
       "      <th>number_of_downvotes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>datetime</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>year</th>\n",
       "      <th>title_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>title_sent_neg</th>\n",
       "      <th>title_sent_neu</th>\n",
       "      <th>title_sent_pos</th>\n",
       "      <th>title_sent_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.333172e+09</td>\n",
       "      <td>and here's a downvote.</td>\n",
       "      <td>funny</td>\n",
       "      <td>1844.0</td>\n",
       "      <td>32657.0</td>\n",
       "      <td>30813.0</td>\n",
       "      <td>63470.0</td>\n",
       "      <td>2012-03-31 05:40:39</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2012</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.333178e+09</td>\n",
       "      <td>expectation</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2012-03-31 07:16:01</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2012</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.333200e+09</td>\n",
       "      <td>downvote</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>23.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2012-03-31 13:18:33</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.333252e+09</td>\n",
       "      <td>every time i downvote something</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2012-04-01 03:52:10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.333273e+09</td>\n",
       "      <td>downvote \"dies irae\"</td>\n",
       "      <td>GifSound</td>\n",
       "      <td>49.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2012-04-01 09:35:54</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unixtime                            title subreddit   score  \\\n",
       "0  1.333172e+09           and here's a downvote.     funny  1844.0   \n",
       "1  1.333178e+09                      expectation  GifSound    23.0   \n",
       "2  1.333200e+09                         downvote  GifSound    23.0   \n",
       "3  1.333252e+09  every time i downvote something  GifSound     2.0   \n",
       "4  1.333273e+09             downvote \"dies irae\"  GifSound    49.0   \n",
       "\n",
       "   number_of_upvotes  number_of_downvotes  total_votes            datetime  \\\n",
       "0            32657.0              30813.0      63470.0 2012-03-31 05:40:39   \n",
       "1               29.0                  6.0         35.0 2012-03-31 07:16:01   \n",
       "2               32.0                  9.0         41.0 2012-03-31 13:18:33   \n",
       "3                6.0                  4.0         10.0 2012-04-01 03:52:10   \n",
       "4               57.0                  8.0         65.0 2012-04-01 09:35:54   \n",
       "\n",
       "   hour  dayofweek  year  title_length  word_count  title_sent_neg  \\\n",
       "0     5          5  2012            22           4             0.0   \n",
       "1     7          5  2012            11           1             0.0   \n",
       "2    13          5  2012             8           1             0.0   \n",
       "3     3          6  2012            31           5             0.0   \n",
       "4     9          6  2012            20           3             0.0   \n",
       "\n",
       "   title_sent_neu  title_sent_pos  title_sent_compound  \n",
       "0             1.0             0.0                  0.0  \n",
       "1             1.0             0.0                  0.0  \n",
       "2             1.0             0.0                  0.0  \n",
       "3             1.0             0.0                  0.0  \n",
       "4             1.0             0.0                  0.0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_use = [\n",
    "    'unixtime', 'title', 'subreddit', 'score',\n",
    "    'number_of_upvotes', 'number_of_downvotes', 'total_votes',\n",
    "    'datetime', 'hour', 'dayofweek', 'year',\n",
    "    'title_length', 'word_count', 'title_sent_neg', 'title_sent_neu', 'title_sent_pos', 'title_sent_compound'\n",
    "]\n",
    "\n",
    "df = df[cols_to_use]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELING\n",
    "\n",
    "# make sure to drop total_votes, upvotes, downvotes, number_of_commetns\n",
    "# lets drop all subreddits with < 20 posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Split helper (so you always use the same split)\n",
    "def train_val_test_split(X, y, test_size=0.2, val_size=0.1, random_state=158):\n",
    "    \"\"\"\n",
    "    Splits X, y into train/val/test.\n",
    "    val_size is the fraction of the *original* data.\n",
    "    \"\"\"\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    val_frac_of_temp = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_frac_of_temp, random_state=random_state\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def eval_regression(y_true, y_pred, name=\"Model\"):\n",
    "\n",
    "    # Evaluate a regression model using MAE (primary metric).\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f\"{name}\")\n",
    "    print(f\"  MAE : {mae:.4f}\")\n",
    "    return {\"model\": name, \"mae\": mae}\n",
    "\n",
    "\n",
    "# 3. Convenience wrapper: fit model + evaluate on a given split\n",
    "def fit_and_evaluate(model, X_train, y_train, X_val, y_val, name=None):\n",
    "    \"\"\"\n",
    "    Fits the model on (X_train, y_train) and evaluates on (X_val, y_val).\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    model_name = name or model.__class__.__name__\n",
    "    scores = eval_regression(y_val, y_pred, name=model_name)\n",
    "    return model, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 1\n",
    "\n",
    "Predict Global Mean\n",
    "\n",
    "MAE : 320.6778\n",
    "\n",
    "## Baseline 2\n",
    "\n",
    "Predict Subreddit mean, if DNE, fall back to global mean\n",
    "\n",
    "MAE : 307.3176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global mean score (train): 233.1191\n",
      "Baseline: Global Mean\n",
      "  MAE : 320.6778\n",
      "Baseline: Subreddit Mean + Global Fallback\n",
      "  MAE : 307.3176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'Baseline: Subreddit Mean + Global Fallback',\n",
       " 'mae': 307.31759431661175}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    'title', 'subreddit',\n",
    "    'datetime', 'hour', 'dayofweek', 'year',\n",
    "    'title_length', 'word_count'\n",
    "]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['score']   # or df['score_log'] if you later transform it\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y)\n",
    "\n",
    "# ---- Baseline 1: Global mean predictor ----\n",
    "global_mean = y_train.mean()\n",
    "print(f\"Global mean score (train): {global_mean:.4f}\")\n",
    "\n",
    "# constant prediction = global mean\n",
    "y_val_pred_global = np.full_like(y_val, fill_value=global_mean, dtype=float)\n",
    "eval_regression(y_val, y_val_pred_global, name=\"Baseline: Global Mean\")\n",
    "\n",
    "\n",
    "# ---- Baseline 2: Subreddit mean (with fallback to global mean) ----\n",
    "\n",
    "# compute mean score per subreddit on TRAIN ONLY\n",
    "train_df_for_means = pd.DataFrame({\n",
    "    'subreddit': X_train['subreddit'].values,\n",
    "    'score': y_train.values\n",
    "})\n",
    "subreddit_means = train_df_for_means.groupby('subreddit')['score'].mean()\n",
    "\n",
    "def predict_subreddit_mean(X, subreddit_means, global_mean):\n",
    "    \"\"\"\n",
    "    For each row in X, predict the mean score of its subreddit,\n",
    "    falling back to the global mean if the subreddit was unseen in training.\n",
    "    \"\"\"\n",
    "    return X['subreddit'].map(subreddit_means).fillna(global_mean).values\n",
    "\n",
    "# predictions on validation set\n",
    "y_val_pred_sub = predict_subreddit_mean(X_val, subreddit_means, global_mean)\n",
    "eval_regression(y_val, y_val_pred_sub, name=\"Baseline: Subreddit Mean + Global Fallback\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 1 \n",
    "\n",
    "Random Forest \n",
    "\n",
    "MAE : 329.3844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Metadata + Sentiment)\n",
      "  MAE : 329.3844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'Random Forest (Metadata + Sentiment)', 'mae': 329.38440545877967}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Select the features Random Forest can use\n",
    "# ---------------------------\n",
    "rf_feature_cols = [\n",
    "    'subreddit',\n",
    "    'hour', 'dayofweek', 'year',\n",
    "    'title_length', 'word_count',\n",
    "    'title_sent_neg', 'title_sent_neu', 'title_sent_pos', 'title_sent_compound'\n",
    "]\n",
    "\n",
    "X_rf = df[rf_feature_cols]\n",
    "y = df['score']   # or df['score_log']\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Train/val/test split\n",
    "# ---------------------------\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_rf, y, test_size=0.20, random_state=158\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=158\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Preprocess categorical + numeric\n",
    "# ---------------------------\n",
    "categorical_cols = ['subreddit']\n",
    "numeric_cols = [\n",
    "    'hour', 'dayofweek', 'year',\n",
    "    'title_length', 'word_count',\n",
    "    'title_sent_neg', 'title_sent_neu', 'title_sent_pos', 'title_sent_compound'\n",
    "]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', 'passthrough', numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Define Random Forest model\n",
    "# ---------------------------\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,      # let the model grow\n",
    "    min_samples_split=2,\n",
    "    random_state=158,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Create the modeling pipeline\n",
    "# ---------------------------\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', rf_model)\n",
    "])\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Fit & Evaluate on validation set\n",
    "# ---------------------------\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_val_pred = rf_pipeline.predict(X_val)\n",
    "\n",
    "eval_regression(y_val, y_val_pred, name=\"Random Forest (Metadata + Sentiment)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 2 \n",
    "\n",
    "Ridge (TF-IDF + Metadata + Sentiment)\n",
    "\n",
    "MAE: 311.7044281224904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge (TF-IDF + Metadata + Sentiment)\n",
      "  MAE : 311.7044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'Ridge (TF-IDF + Metadata + Sentiment)', 'mae': 311.7044281224904}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Define X and y\n",
    "# ---------------------------\n",
    "\n",
    "ridge_feature_cols = [\n",
    "    'title', 'subreddit',\n",
    "    'hour', 'dayofweek', 'year',\n",
    "    'title_length', 'word_count',\n",
    "    'title_sent_neg', 'title_sent_neu', 'title_sent_pos', 'title_sent_compound'\n",
    "]\n",
    "\n",
    "X = df[ridge_feature_cols]\n",
    "y = df['score']          # or df['score_log'] if you transform\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Train / val / test split (80/10/10)\n",
    "# ---------------------------\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=158\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=158\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3. ColumnTransformer for preprocessing\n",
    "# ---------------------------\n",
    "\n",
    "text_col = 'title'\n",
    "cat_cols = ['subreddit']\n",
    "num_cols = [\n",
    "    'hour', 'dayofweek', 'year',\n",
    "    'title_length', 'word_count',\n",
    "    'title_sent_neg', 'title_sent_neu', 'title_sent_pos', 'title_sent_compound'\n",
    "]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # TF-IDF on title text\n",
    "        ('text', TfidfVectorizer(\n",
    "            max_features=20000,\n",
    "            ngram_range=(1, 2),   # unigrams + bigrams\n",
    "            min_df=5\n",
    "        ), text_col),\n",
    "\n",
    "        # One-hot encode subreddit\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "\n",
    "        # Scale numeric + sentiment features\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Ridge regression model\n",
    "# ---------------------------\n",
    "\n",
    "ridge = Ridge(alpha=1.0, random_state=158)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Full pipeline: preprocess -> model\n",
    "# ---------------------------\n",
    "\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', ridge),\n",
    "])\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Fit & evaluate on validation set\n",
    "# ---------------------------\n",
    "\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "y_val_pred = ridge_pipeline.predict(X_val)\n",
    "\n",
    "eval_regression(y_val, y_val_pred, name=\"Ridge (TF-IDF + Metadata + Sentiment)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 3 \n",
    "\n",
    "Ridge (TF-IDF, tuned alpha, log-shift target)\n",
    "\n",
    "MAE: 253.75196347566893\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum score in data: -264.0\n",
      "Any NaNs in y_log? 0\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n",
      "Best params: {'model__alpha': 30}\n",
      "Best CV RMSE (log space): 0.5831789902426164\n",
      "Ridge (TF-IDF, tuned alpha, log-shift target)\n",
      "  MAE : 253.7520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'Ridge (TF-IDF, tuned alpha, log-shift target)',\n",
       " 'mae': 253.75196347566893}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ------------- 1. Define X and log-transformed y (with shift) -------------\n",
    "\n",
    "ridge_feature_cols = [\n",
    "    'title', 'subreddit',\n",
    "    'hour', 'dayofweek', 'year',\n",
    "    'title_length', 'word_count',\n",
    "    'title_sent_neg', 'title_sent_neu', 'title_sent_pos', 'title_sent_compound'\n",
    "]\n",
    "\n",
    "X = df[ridge_feature_cols]\n",
    "\n",
    "# raw target\n",
    "y_raw = df['score']\n",
    "\n",
    "# handle negative scores by shifting before log\n",
    "min_score = y_raw.min()\n",
    "print(\"Minimum score in data:\", min_score)\n",
    "\n",
    "# shift so that the minimum becomes 1, then log-transform\n",
    "y_shifted = y_raw - min_score + 1\n",
    "y_log = np.log(y_shifted)\n",
    "\n",
    "# quick sanity check\n",
    "print(\"Any NaNs in y_log?\", np.isnan(y_log).sum())\n",
    "\n",
    "\n",
    "# ------------- 2. Train / val / test split (keep raw + log aligned) -------------\n",
    "\n",
    "X_train, X_temp, y_train_log, y_temp_log, y_train_raw, y_temp_raw = train_test_split(\n",
    "    X, y_log, y_raw, test_size=0.20, random_state=158\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val_log, y_test_log, y_val_raw, y_test_raw = train_test_split(\n",
    "    X_temp, y_temp_log, y_temp_raw, test_size=0.50, random_state=158\n",
    ")\n",
    "\n",
    "\n",
    "# ------------- 3. Preprocessing (TF-IDF + OHE + scaling) -------------\n",
    "\n",
    "text_col = 'title'\n",
    "cat_cols = ['subreddit']\n",
    "num_cols = [\n",
    "    'hour', 'dayofweek', 'year',\n",
    "    'title_length', 'word_count',\n",
    "    'title_sent_neg', 'title_sent_neu', 'title_sent_pos', 'title_sent_compound'\n",
    "]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(\n",
    "            max_features=20000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=5\n",
    "        ), text_col),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# ------------- 4. Base Ridge pipeline (we'll tune alpha) -------------\n",
    "\n",
    "base_ridge = Ridge(random_state=158)\n",
    "\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', base_ridge),\n",
    "])\n",
    "\n",
    "\n",
    "# ------------- 5. Hyperparameter tuning for alpha (on log target) -------------\n",
    "\n",
    "param_grid = {\n",
    "    'model__alpha': [0.01, 0.1, 1, 3, 10, 30, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    ridge_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',  # still fine to tune on RMSE in log space\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train_log)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV RMSE (log space):\", -grid.best_score_)\n",
    "\n",
    "\n",
    "# ------------- 6. Evaluate best model on validation set (in RAW score space) -------------\n",
    "\n",
    "best_ridge = grid.best_estimator_\n",
    "\n",
    "# predict in log(shifted score) space\n",
    "y_val_pred_log = best_ridge.predict(X_val)\n",
    "\n",
    "# invert: shifted = exp(log_pred), then unshift back to original score scale\n",
    "y_val_pred_shifted = np.exp(y_val_pred_log)\n",
    "y_val_pred_raw = y_val_pred_shifted + min_score - 1\n",
    "\n",
    "eval_regression(y_val_raw, y_val_pred_raw, name=\"Ridge (TF-IDF, tuned alpha, log-shift target)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
