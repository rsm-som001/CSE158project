{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\n",
    "#     \"/Users/andrewchen/CSE158/Assignment_2/redditSubmissions.csv.gz\",\n",
    "#     compression=\"gzip\",\n",
    "#     on_bad_lines=\"skip\",\n",
    "#     engine=\"python\"\n",
    "# )\n",
    "\n",
    "# df\n",
    "df = pd.read_csv(\"BenEDA/reddit_prepared.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEN'S EDA IS RIGHT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## however many cells needed. Note the structure of the data.\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Load data\n",
    "# df = pd.read_csv(\"BenEDA/reddit_prepared.csv\")\n",
    "# df[\"post_date\"] = pd.to_datetime(df[\"post_date\"])\n",
    "\n",
    "# # Set style\n",
    "# sns.set_style(\"whitegrid\")\n",
    "# plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "\n",
    "# # Create subplots\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "# fig.suptitle(\"Reddit Post Analysis Dashboard\", fontsize=16, fontweight=\"bold\", y=1.00)\n",
    "\n",
    "# # 1. Best Hours to Post\n",
    "# ax1 = axes[0, 0]\n",
    "# hour_scores = df.groupby(\"hour\")[\"score\"].mean().sort_index()\n",
    "# bars = ax1.bar(hour_scores.index, hour_scores.values, color=\"steelblue\", alpha=0.7)\n",
    "# max_hour = int(hour_scores.idxmax())\n",
    "# bars[max_hour].set_color(\"red\")\n",
    "# bars[max_hour].set_alpha(1.0)\n",
    "# ax1.axvline(x=17, color=\"red\", linestyle=\"--\", alpha=0.3, linewidth=2)\n",
    "# ax1.axvline(x=2, color=\"orange\", linestyle=\"--\", alpha=0.3, linewidth=2)\n",
    "# ax1.set_xlabel(\"Hour of Day\")\n",
    "# ax1.set_ylabel(\"Average Score\")\n",
    "# ax1.set_title(f\"Best Time to Post: {max_hour}:00 (5pm=17, 2am=2)\")\n",
    "# ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# # 2. Day of Week Pattern\n",
    "# ax2 = axes[0, 1]\n",
    "# day_names = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "# dow_scores = df.groupby(\"day_of_week\")[\"score\"].mean()\n",
    "# colors = [\"steelblue\"] * 5 + [\"coral\", \"coral\"]\n",
    "# ax2.bar(range(7), dow_scores.values, color=colors, alpha=0.7)\n",
    "# ax2.set_xticks(range(7))\n",
    "# ax2.set_xticklabels(day_names)\n",
    "# ax2.set_ylabel(\"Average Score\")\n",
    "# ax2.set_title(\"Weekend vs Weekday\")\n",
    "# ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# # 3. Top Subreddits\n",
    "# ax3 = axes[0, 2]\n",
    "# top_subs = df[\"subreddit\"].value_counts().head(10)\n",
    "# ax3.barh(range(10), top_subs.values, color=\"teal\", alpha=0.7)\n",
    "# ax3.set_yticks(range(10))\n",
    "# ax3.set_yticklabels(top_subs.index, fontsize=9)\n",
    "# ax3.set_xlabel(\"Number of Posts\")\n",
    "# ax3.set_title(\"Most Active Subreddits\")\n",
    "# ax3.invert_yaxis()\n",
    "# ax3.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# # 4. Title Length vs Score\n",
    "# ax4 = axes[1, 0]\n",
    "# sample = df.sample(min(3000, len(df)))\n",
    "# ax4.scatter(sample[\"title_length\"], sample[\"score\"], alpha=0.05, s=20, color=\"purple\")\n",
    "# ax4.set_xlabel(\"Title Length (characters)\")\n",
    "# ax4.set_ylabel(\"Score\")\n",
    "# ax4.set_title(\"Title Length Impact\")\n",
    "# ax4.set_ylim(0, df[\"score\"].quantile(0.95))\n",
    "# ax4.grid(alpha=0.3)\n",
    "\n",
    "# # Add trend line\n",
    "# z = np.polyfit(df[\"title_length\"].dropna(), df[\"score\"], 1)\n",
    "# p = np.poly1d(z)\n",
    "# x_trend = np.linspace(df[\"title_length\"].min(), df[\"title_length\"].max(), 100)\n",
    "# ax4.plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# # 5. Score Distribution\n",
    "# ax5 = axes[1, 1]\n",
    "# ax5.hist(df[\"score\"], bins=50, color=\"steelblue\", alpha=0.7, edgecolor=\"black\")\n",
    "# ax5.axvline(\n",
    "#     df[\"score\"].mean(),\n",
    "#     color=\"red\",\n",
    "#     linestyle=\"--\",\n",
    "#     linewidth=2,\n",
    "#     label=f\"Mean: {df['score'].mean():.0f}\",\n",
    "# )\n",
    "# ax5.axvline(\n",
    "#     df[\"score\"].median(),\n",
    "#     color=\"orange\",\n",
    "#     linestyle=\"--\",\n",
    "#     linewidth=2,\n",
    "#     label=f\"Median: {df['score'].median():.0f}\",\n",
    "# )\n",
    "# ax5.set_xlabel(\"Score\")\n",
    "# ax5.set_ylabel(\"Frequency (log scale)\")\n",
    "# ax5.set_title(\"Score Distribution\")\n",
    "# ax5.set_xlim(0, df[\"score\"].quantile(0.95))\n",
    "# ax5.set_yscale(\"log\")\n",
    "# ax5.legend()\n",
    "# ax5.grid(alpha=0.3)\n",
    "\n",
    "# # 6. Correlation Heatmap (Top Features)\n",
    "# ax6 = axes[1, 2]\n",
    "# key_features = [\n",
    "#     \"score\",\n",
    "#     \"hour\",\n",
    "#     \"day_of_week\",\n",
    "#     \"title_length\",\n",
    "#     \"has_question_mark\",\n",
    "#     \"positive_words\",\n",
    "#     \"author_avg_score\",\n",
    "# ]\n",
    "# corr_matrix = df[key_features].corr()\n",
    "# sns.heatmap(\n",
    "#     corr_matrix,\n",
    "#     annot=True,\n",
    "#     fmt=\".2f\",\n",
    "#     cmap=\"coolwarm\",\n",
    "#     center=0,\n",
    "#     square=True,\n",
    "#     linewidths=1,\n",
    "#     ax=ax6,\n",
    "#     cbar_kws={\"shrink\": 0.8},\n",
    "# )\n",
    "# ax6.set_title(\"Feature Correlations\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(\"\\nKey Findings:\")\n",
    "# print(f\"- Best hour to post: {max_hour}:00\")\n",
    "# print(f\"- Best day: {day_names[int(dow_scores.idxmax())]}\")\n",
    "# print(f\"- Average score: {df['score'].mean():.1f}\")\n",
    "# print(f\"- Median score: {df['score'].median():.1f}\")\n",
    "# print(f\"- Title length correlation: {df['title_length'].corr(df['score']):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Validation\n",
    "\n",
    "Before building any models, we first validate that the core numeric fields in the dataset are internally consistent and that there are no obvious data quality issues.\n",
    "\n",
    "**Checks performed:**\n",
    "\n",
    "1. **Score consistency**\n",
    "   We verify that the Reddit `score` field matches the definition:\n",
    "      `score` = `number_of_upvotes` - `number_of_downvotes`\n",
    "\n",
    "2. **Non-negative votes**\n",
    "\n",
    "   We confirm that `number_of_upvotes` and `number_of_downvotes` are never negative.  \n",
    "\n",
    "3. **Duplicated posts**\n",
    "\n",
    "   Using `reddit_id` as the unique identifier of a submission, we check for duplicated `reddit_id`s.  \n",
    "\n",
    "4. **Missing values**\n",
    "\n",
    "   Finally, we compute the total number of missing values in the dataset\n",
    "      - The `username` column has 20,260 missing values, but will not be considered in our model\n",
    "      - Every other column only has 1 missing value on the same row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score consistency check (should be 0): 0.0\n",
      "Number of rows with negative votes (should be 0): 0\n",
      "Number of duplicated reddit_id values: 93\n",
      "#image_id                    0\n",
      "unixtime                     0\n",
      "rawtime                      1\n",
      "title                        1\n",
      "total_votes                  0\n",
      "reddit_id                    1\n",
      "number_of_upvotes            0\n",
      "subreddit                    0\n",
      "number_of_downvotes          0\n",
      "localtime                    0\n",
      "score                        0\n",
      "number_of_comments           0\n",
      "username                     0\n",
      "post_date                    1\n",
      "hour                         0\n",
      "day_of_week                  0\n",
      "is_weekend                   0\n",
      "month                        0\n",
      "day                          0\n",
      "time_of_day                  0\n",
      "title_length                 0\n",
      "title_word_count             0\n",
      "avg_word_length              0\n",
      "has_question_mark            0\n",
      "has_exclamation              0\n",
      "num_punctuation              0\n",
      "has_caps                     0\n",
      "all_caps_words               0\n",
      "positive_words               0\n",
      "negative_words               0\n",
      "author_total_score           0\n",
      "author_avg_score             0\n",
      "author_post_count            0\n",
      "author_avg_comments          0\n",
      "author_avg_upvotes           0\n",
      "is_deleted_user              0\n",
      "subreddit_avg_score          0\n",
      "subreddit_median_score       0\n",
      "subreddit_score_std          0\n",
      "subreddit_avg_total_votes    0\n",
      "subreddit_avg_comments       0\n",
      "subreddit_avg_upvotes        0\n",
      "subreddit_activity_level     1\n",
      "upvote_ratio                 0\n",
      "comment_to_vote_ratio        0\n",
      "controversy_score            0\n",
      "is_viral                     0\n",
      "high_engagement              0\n",
      "popularity_category          1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Score = upvotes - downvotes\n",
    "score_diff = (\n",
    "    (df[\"number_of_upvotes\"] - df[\"number_of_downvotes\"] - df[\"score\"]).abs().sum()\n",
    ")\n",
    "print(\"Score consistency check (should be 0):\", score_diff)\n",
    "\n",
    "# Non-negative votes\n",
    "\n",
    "neg_votes = ((df[\"number_of_upvotes\"] < 0) | (df[\"number_of_downvotes\"] < 0)).sum()\n",
    "print(\"Number of rows with negative votes (should be 0):\", neg_votes)\n",
    "\n",
    "dup_count = df[\"reddit_id\"].duplicated().sum()\n",
    "print(\"Number of duplicated reddit_id values:\", dup_count)\n",
    "\n",
    "# Missing Values\n",
    "df_nan = df.isna().sum()\n",
    "print(df_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated reddit_id values: 0\n",
      "Number of NANs: 0\n"
     ]
    }
   ],
   "source": [
    "# Casting\n",
    "df[\"unixtime\"] = pd.to_numeric(df[\"unixtime\"], errors=\"coerce\")\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"unixtime\"], unit=\"s\")\n",
    "df[\"total_votes\"] = df[\"total_votes\"].astype(float)\n",
    "df[\"number_of_upvotes\"] = df[\"number_of_upvotes\"].astype(float)\n",
    "df[\"number_of_downvotes\"] = df[\"number_of_downvotes\"].astype(float)\n",
    "df[\"score\"] = df[\"score\"].astype(float)\n",
    "\n",
    "# Duplicate Handling\n",
    "df = df.drop_duplicates(subset=[\"reddit_id\"])\n",
    "dup_reddit_ids = df[\"reddit_id\"].duplicated().sum()\n",
    "print(\"Number of duplicated reddit_id values:\", dup_reddit_ids)\n",
    "\n",
    "# Missing Values Handling\n",
    "df = df.drop(columns=[\"username\"])\n",
    "df = df.dropna()\n",
    "print(\"Number of NANs:\", df.isna().sum().sum())\n",
    "\n",
    "\n",
    "def clean_title(text):\n",
    "    # 1. Ensure string\n",
    "    s = str(text)\n",
    "    # 2. Unescape HTML entities: &quot; &amp; &lt; &gt; etc.\n",
    "    s = html.unescape(s)\n",
    "    # 3. Strip leading/trailing whitespace\n",
    "    s = s.strip()\n",
    "    # 4. Collapse multiple spaces/newlines into a single space\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # 5. (Optional) lowercase for modeling\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "\n",
    "df[\"title_clean\"] = df[\"title\"].apply(clean_title)\n",
    "df[\"title\"] = df[\"title_clean\"]\n",
    "df.drop(columns=[\"title_clean\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Because Reddit scores are extremely heavy-tailed, even after a log-transform the top 1–5% of posts still had disproportionate influence and caused numerically unstable predictions.\n",
    "\n",
    "## We removed the top 5% of scores to make the distribution more well-behaved. After this, log-space MAE improved significantly and raw-space MAE became interpretable and stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "p95 = df[\"score\"].quantile(0.95)\n",
    "df = df[df[\"score\"] <= p95].copy()\n",
    "\n",
    "\n",
    "# this move massively improved our model\n",
    "\n",
    "# i dont have a graph but i'm pretty sure the values towards the top are astronomically large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125572, 49)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#image_id</th>\n",
       "      <th>unixtime</th>\n",
       "      <th>rawtime</th>\n",
       "      <th>title</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>number_of_upvotes</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>number_of_downvotes</th>\n",
       "      <th>localtime</th>\n",
       "      <th>...</th>\n",
       "      <th>subreddit_avg_comments</th>\n",
       "      <th>subreddit_avg_upvotes</th>\n",
       "      <th>subreddit_activity_level</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>comment_to_vote_ratio</th>\n",
       "      <th>controversy_score</th>\n",
       "      <th>is_viral</th>\n",
       "      <th>high_engagement</th>\n",
       "      <th>popularity_category</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21674</td>\n",
       "      <td>1.217219e+09</td>\n",
       "      <td>2008-07-28T11:26:54-07:00</td>\n",
       "      <td>the tennis ball haircut</td>\n",
       "      <td>705.0</td>\n",
       "      <td>6ttui</td>\n",
       "      <td>553.0</td>\n",
       "      <td>offbeat</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1.217244e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>127.780488</td>\n",
       "      <td>1581.121951</td>\n",
       "      <td>large</td>\n",
       "      <td>0.784397</td>\n",
       "      <td>0.082270</td>\n",
       "      <td>0.274368</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>viral</td>\n",
       "      <td>2008-07-28 04:26:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13810</td>\n",
       "      <td>1.221583e+09</td>\n",
       "      <td>2008-09-16T23:41:02-07:00</td>\n",
       "      <td>library parking garage [pic]</td>\n",
       "      <td>487.0</td>\n",
       "      <td>71vwb</td>\n",
       "      <td>431.0</td>\n",
       "      <td>offbeat</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.221608e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>127.780488</td>\n",
       "      <td>1581.121951</td>\n",
       "      <td>large</td>\n",
       "      <td>0.885010</td>\n",
       "      <td>0.133470</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>viral</td>\n",
       "      <td>2008-09-16 16:41:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21236</td>\n",
       "      <td>1.227545e+09</td>\n",
       "      <td>2008-11-24T23:44:22-07:00</td>\n",
       "      <td>shuttle mission [pic]</td>\n",
       "      <td>175.0</td>\n",
       "      <td>7fidh</td>\n",
       "      <td>151.0</td>\n",
       "      <td>space</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.227570e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>57.862385</td>\n",
       "      <td>956.009174</td>\n",
       "      <td>large</td>\n",
       "      <td>0.862857</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>high</td>\n",
       "      <td>2008-11-24 16:44:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8130</td>\n",
       "      <td>1.228388e+09</td>\n",
       "      <td>2008-12-04T17:57:12-07:00</td>\n",
       "      <td>what happens when a person falls in love with ...</td>\n",
       "      <td>518.0</td>\n",
       "      <td>7hh3w</td>\n",
       "      <td>413.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.228413e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>36.924980</td>\n",
       "      <td>1345.375590</td>\n",
       "      <td>large</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.152510</td>\n",
       "      <td>0.253623</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>viral</td>\n",
       "      <td>2008-12-04 10:57:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3177</td>\n",
       "      <td>1.230905e+09</td>\n",
       "      <td>2009-01-02T21:03:29.546565-07:00</td>\n",
       "      <td>a map of all the undersea internet cables in t...</td>\n",
       "      <td>333.0</td>\n",
       "      <td>7n2sq</td>\n",
       "      <td>293.0</td>\n",
       "      <td>geek</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.230930e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>78.811765</td>\n",
       "      <td>1323.094118</td>\n",
       "      <td>large</td>\n",
       "      <td>0.879880</td>\n",
       "      <td>0.204204</td>\n",
       "      <td>0.136054</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>viral</td>\n",
       "      <td>2009-01-02 14:03:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    #image_id      unixtime                           rawtime  \\\n",
       "0       21674  1.217219e+09         2008-07-28T11:26:54-07:00   \n",
       "1       13810  1.221583e+09         2008-09-16T23:41:02-07:00   \n",
       "4       21236  1.227545e+09         2008-11-24T23:44:22-07:00   \n",
       "6        8130  1.228388e+09         2008-12-04T17:57:12-07:00   \n",
       "10       3177  1.230905e+09  2009-01-02T21:03:29.546565-07:00   \n",
       "\n",
       "                                                title  total_votes reddit_id  \\\n",
       "0                             the tennis ball haircut        705.0     6ttui   \n",
       "1                        library parking garage [pic]        487.0     71vwb   \n",
       "4                               shuttle mission [pic]        175.0     7fidh   \n",
       "6   what happens when a person falls in love with ...        518.0     7hh3w   \n",
       "10  a map of all the undersea internet cables in t...        333.0     7n2sq   \n",
       "\n",
       "    number_of_upvotes subreddit  number_of_downvotes     localtime  ...  \\\n",
       "0               553.0   offbeat                152.0  1.217244e+09  ...   \n",
       "1               431.0   offbeat                 56.0  1.221608e+09  ...   \n",
       "4               151.0     space                 24.0  1.227570e+09  ...   \n",
       "6               413.0     funny                105.0  1.228413e+09  ...   \n",
       "10              293.0      geek                 40.0  1.230930e+09  ...   \n",
       "\n",
       "    subreddit_avg_comments  subreddit_avg_upvotes subreddit_activity_level  \\\n",
       "0               127.780488            1581.121951                    large   \n",
       "1               127.780488            1581.121951                    large   \n",
       "4                57.862385             956.009174                    large   \n",
       "6                36.924980            1345.375590                    large   \n",
       "10               78.811765            1323.094118                    large   \n",
       "\n",
       "    upvote_ratio  comment_to_vote_ratio  controversy_score  is_viral  \\\n",
       "0       0.784397               0.082270           0.274368         1   \n",
       "1       0.885010               0.133470           0.129630         1   \n",
       "4       0.862857               0.120000           0.157895         0   \n",
       "6       0.797297               0.152510           0.253623         1   \n",
       "10      0.879880               0.204204           0.136054         1   \n",
       "\n",
       "    high_engagement popularity_category            datetime  \n",
       "0                 1               viral 2008-07-28 04:26:54  \n",
       "1                 1               viral 2008-09-16 16:41:02  \n",
       "4                 1                high 2008-11-24 16:44:22  \n",
       "6                 1               viral 2008-12-04 10:57:12  \n",
       "10                1               viral 2009-01-02 14:03:29  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "df[\"dayofweek\"] = df[\"datetime\"].dt.dayofweek\n",
    "df[\"month\"] = df[\"datetime\"].dt.month\n",
    "df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "\n",
    "df[\"title\"] = df[\"title\"].astype(str).str.strip()\n",
    "df[\"title_length\"] = df[\"title\"].apply(len)\n",
    "df[\"word_count\"] = df[\"title\"].apply(lambda x: len(x.split()))\n",
    "df[\"is_question\"] = df[\"title\"].str.contains(\"?\", regex=False).astype(int)\n",
    "df[\"is_exclamation\"] = df[\"title\"].str.contains(\"!\", regex=False).astype(int)\n",
    "\n",
    "\n",
    "# Adding subreddit stats - I did not include subreddit stats as I believe it leaks the target variable\n",
    "# subreddit_stats = df.groupby('subreddit')['score'].agg(\n",
    "#     subreddit_mean_score='mean',\n",
    "#     subreddit_post_count='count'\n",
    "# ).reset_index()\n",
    "# df = df.merge(subreddit_stats, on='subreddit', how='left')\n",
    "\n",
    "# Adding sentiment analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def add_title_sentiment(df):\n",
    "    titles = df[\"title\"].astype(str)\n",
    "\n",
    "    scores = titles.apply(sia.polarity_scores)\n",
    "    scores_df = scores.apply(pd.Series)\n",
    "\n",
    "    df[\"title_sent_neg\"] = scores_df[\"neg\"]\n",
    "    df[\"title_sent_neu\"] = scores_df[\"neu\"]\n",
    "    df[\"title_sent_pos\"] = scores_df[\"pos\"]\n",
    "    df[\"title_sent_compound\"] = scores_df[\"compound\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "# add new column\n",
    "df = add_title_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#image_id</th>\n",
       "      <th>unixtime</th>\n",
       "      <th>rawtime</th>\n",
       "      <th>title</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>number_of_upvotes</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>number_of_downvotes</th>\n",
       "      <th>localtime</th>\n",
       "      <th>...</th>\n",
       "      <th>popularity_category</th>\n",
       "      <th>datetime</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>word_count</th>\n",
       "      <th>is_question</th>\n",
       "      <th>is_exclamation</th>\n",
       "      <th>title_sent_neg</th>\n",
       "      <th>title_sent_neu</th>\n",
       "      <th>title_sent_pos</th>\n",
       "      <th>title_sent_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21674</td>\n",
       "      <td>1.217219e+09</td>\n",
       "      <td>2008-07-28T11:26:54-07:00</td>\n",
       "      <td>the tennis ball haircut</td>\n",
       "      <td>705.0</td>\n",
       "      <td>6ttui</td>\n",
       "      <td>553.0</td>\n",
       "      <td>offbeat</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1.217244e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>viral</td>\n",
       "      <td>2008-07-28 04:26:54</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13810</td>\n",
       "      <td>1.221583e+09</td>\n",
       "      <td>2008-09-16T23:41:02-07:00</td>\n",
       "      <td>library parking garage [pic]</td>\n",
       "      <td>487.0</td>\n",
       "      <td>71vwb</td>\n",
       "      <td>431.0</td>\n",
       "      <td>offbeat</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.221608e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>viral</td>\n",
       "      <td>2008-09-16 16:41:02</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21236</td>\n",
       "      <td>1.227545e+09</td>\n",
       "      <td>2008-11-24T23:44:22-07:00</td>\n",
       "      <td>shuttle mission [pic]</td>\n",
       "      <td>175.0</td>\n",
       "      <td>7fidh</td>\n",
       "      <td>151.0</td>\n",
       "      <td>space</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.227570e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>high</td>\n",
       "      <td>2008-11-24 16:44:22</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8130</td>\n",
       "      <td>1.228388e+09</td>\n",
       "      <td>2008-12-04T17:57:12-07:00</td>\n",
       "      <td>what happens when a person falls in love with ...</td>\n",
       "      <td>518.0</td>\n",
       "      <td>7hh3w</td>\n",
       "      <td>413.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.228413e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>viral</td>\n",
       "      <td>2008-12-04 10:57:12</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.7430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3177</td>\n",
       "      <td>1.230905e+09</td>\n",
       "      <td>2009-01-02T21:03:29.546565-07:00</td>\n",
       "      <td>a map of all the undersea internet cables in t...</td>\n",
       "      <td>333.0</td>\n",
       "      <td>7n2sq</td>\n",
       "      <td>293.0</td>\n",
       "      <td>geek</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.230930e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>viral</td>\n",
       "      <td>2009-01-02 14:03:29</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132270</th>\n",
       "      <td>9300</td>\n",
       "      <td>1.359092e+09</td>\n",
       "      <td>2013-01-25T05:39:53+00:00</td>\n",
       "      <td>googled \"reddit worthy pictures\" this was the ...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>178pcg</td>\n",
       "      <td>7.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.359092e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>2013-01-25 05:39:53</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132271</th>\n",
       "      <td>4229</td>\n",
       "      <td>1.359093e+09</td>\n",
       "      <td>2013-01-25T05:43:34+00:00</td>\n",
       "      <td>how about now?</td>\n",
       "      <td>26.0</td>\n",
       "      <td>178pjr</td>\n",
       "      <td>9.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.359093e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>2013-01-25 05:43:34</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132272</th>\n",
       "      <td>4229</td>\n",
       "      <td>1.359093e+09</td>\n",
       "      <td>2013-01-25T05:54:43+00:00</td>\n",
       "      <td>why wait 3 months?</td>\n",
       "      <td>15.0</td>\n",
       "      <td>178q6d</td>\n",
       "      <td>5.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.359093e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>2013-01-25 05:54:43</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132273</th>\n",
       "      <td>4518</td>\n",
       "      <td>1.359094e+09</td>\n",
       "      <td>2013-01-25T06:05:20+00:00</td>\n",
       "      <td>reddit's reaction to the majority of my posts</td>\n",
       "      <td>9.0</td>\n",
       "      <td>178qqd</td>\n",
       "      <td>6.0</td>\n",
       "      <td>funny</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.359094e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>2013-01-25 06:05:20</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132274</th>\n",
       "      <td>5325</td>\n",
       "      <td>1.359095e+09</td>\n",
       "      <td>2013-01-25T06:27:33+00:00</td>\n",
       "      <td>the superman that almost was</td>\n",
       "      <td>18.0</td>\n",
       "      <td>178rw1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>pics</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.359095e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>2013-01-25 06:27:33</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125572 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        #image_id      unixtime                           rawtime  \\\n",
       "0           21674  1.217219e+09         2008-07-28T11:26:54-07:00   \n",
       "1           13810  1.221583e+09         2008-09-16T23:41:02-07:00   \n",
       "4           21236  1.227545e+09         2008-11-24T23:44:22-07:00   \n",
       "6            8130  1.228388e+09         2008-12-04T17:57:12-07:00   \n",
       "10           3177  1.230905e+09  2009-01-02T21:03:29.546565-07:00   \n",
       "...           ...           ...                               ...   \n",
       "132270       9300  1.359092e+09         2013-01-25T05:39:53+00:00   \n",
       "132271       4229  1.359093e+09         2013-01-25T05:43:34+00:00   \n",
       "132272       4229  1.359093e+09         2013-01-25T05:54:43+00:00   \n",
       "132273       4518  1.359094e+09         2013-01-25T06:05:20+00:00   \n",
       "132274       5325  1.359095e+09         2013-01-25T06:27:33+00:00   \n",
       "\n",
       "                                                    title  total_votes  \\\n",
       "0                                 the tennis ball haircut        705.0   \n",
       "1                            library parking garage [pic]        487.0   \n",
       "4                                   shuttle mission [pic]        175.0   \n",
       "6       what happens when a person falls in love with ...        518.0   \n",
       "10      a map of all the undersea internet cables in t...        333.0   \n",
       "...                                                   ...          ...   \n",
       "132270  googled \"reddit worthy pictures\" this was the ...         15.0   \n",
       "132271                                     how about now?         26.0   \n",
       "132272                                 why wait 3 months?         15.0   \n",
       "132273      reddit's reaction to the majority of my posts          9.0   \n",
       "132274                       the superman that almost was         18.0   \n",
       "\n",
       "       reddit_id  number_of_upvotes subreddit  number_of_downvotes  \\\n",
       "0          6ttui              553.0   offbeat                152.0   \n",
       "1          71vwb              431.0   offbeat                 56.0   \n",
       "4          7fidh              151.0     space                 24.0   \n",
       "6          7hh3w              413.0     funny                105.0   \n",
       "10         7n2sq              293.0      geek                 40.0   \n",
       "...          ...                ...       ...                  ...   \n",
       "132270    178pcg                7.0     funny                  8.0   \n",
       "132271    178pjr                9.0     funny                 17.0   \n",
       "132272    178q6d                5.0     funny                 10.0   \n",
       "132273    178qqd                6.0     funny                  3.0   \n",
       "132274    178rw1               12.0      pics                  6.0   \n",
       "\n",
       "           localtime  ...  popularity_category            datetime dayofweek  \\\n",
       "0       1.217244e+09  ...                viral 2008-07-28 04:26:54         0   \n",
       "1       1.221608e+09  ...                viral 2008-09-16 16:41:02         1   \n",
       "4       1.227570e+09  ...                 high 2008-11-24 16:44:22         0   \n",
       "6       1.228413e+09  ...                viral 2008-12-04 10:57:12         3   \n",
       "10      1.230930e+09  ...                viral 2009-01-02 14:03:29         4   \n",
       "...              ...  ...                  ...                 ...       ...   \n",
       "132270  1.359092e+09  ...                  low 2013-01-25 05:39:53         4   \n",
       "132271  1.359093e+09  ...                  low 2013-01-25 05:43:34         4   \n",
       "132272  1.359093e+09  ...                  low 2013-01-25 05:54:43         4   \n",
       "132273  1.359094e+09  ...                  low 2013-01-25 06:05:20         4   \n",
       "132274  1.359095e+09  ...                  low 2013-01-25 06:27:33         4   \n",
       "\n",
       "        word_count  is_question  is_exclamation  title_sent_neg  \\\n",
       "0                4            0               0             0.0   \n",
       "1                4            0               0             0.0   \n",
       "4                3            0               0             0.0   \n",
       "6               11            1               0             0.0   \n",
       "10              11            0               0             0.0   \n",
       "...            ...          ...             ...             ...   \n",
       "132270          12            0               0             0.0   \n",
       "132271           3            1               0             0.0   \n",
       "132272           4            1               0             0.0   \n",
       "132273           8            0               0             0.0   \n",
       "132274           5            0               0             0.0   \n",
       "\n",
       "        title_sent_neu title_sent_pos  title_sent_compound  \n",
       "0                1.000          0.000               0.0000  \n",
       "1                1.000          0.000               0.0000  \n",
       "4                1.000          0.000               0.0000  \n",
       "6                0.588          0.412               0.7430  \n",
       "10               1.000          0.000               0.0000  \n",
       "...                ...            ...                  ...  \n",
       "132270           0.791          0.209               0.4404  \n",
       "132271           1.000          0.000               0.0000  \n",
       "132272           1.000          0.000               0.0000  \n",
       "132273           1.000          0.000               0.0000  \n",
       "132274           1.000          0.000               0.0000  \n",
       "\n",
       "[125572 rows x 57 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up X,y for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (100457, 14)\n",
      "Test shape: (25115, 14)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----- 1. Define X and y -----\n",
    "\n",
    "#Features\n",
    "text_features = [\"title\"]\n",
    "cat_features = [\"subreddit\"]\n",
    "num_features = [\n",
    "    # \"total_votes\",\n",
    "    # \"number_of_upvotes\",\n",
    "    # \"number_of_downvotes\",\n",
    "    # \"number_of_comments\",\n",
    "    \"hour\",\n",
    "    \"dayofweek\",\n",
    "    \"month\",\n",
    "    \"is_weekend\",\n",
    "    \"title_length\",\n",
    "    \"word_count\",\n",
    "    \"is_question\",\n",
    "    \"is_exclamation\",\n",
    "    \"title_sent_neg\",\n",
    "    \"title_sent_neu\",\n",
    "    \"title_sent_pos\",\n",
    "    \"title_sent_compound\",\n",
    "]\n",
    "#Text features: raw text we’ll send into TF-IDF\n",
    "\n",
    "feature_cols = text_features + cat_features + num_features\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "# Target: use log(1 + score) to stabilize heavy tails\n",
    "y_raw = df[\"score\"]\n",
    "min_score = y_raw.min()\n",
    "# shift so that the minimum becomes 1, then log-transform\n",
    "y_shifted = y_raw - min_score + 1\n",
    "y_log = np.log(y_shifted)\n",
    "\n",
    "# ----- 2. Train / test split: 80 / 20 -----\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_log, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 1: Predict Global Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE (log space): 0.3694\n",
      "Baseline MAE (raw score): 174.4228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 1. Baseline in log space\n",
    "mean_pred = y_train.mean()\n",
    "y_test_pred_log = np.full_like(y_test, mean_pred, dtype=float)\n",
    "\n",
    "# 2. Inverse-transform both true and predicted values back to raw score\n",
    "y_test_raw = np.exp(y_test) + min_score - 1\n",
    "y_test_pred_raw = np.exp(y_test_pred_log) + min_score - 1\n",
    "\n",
    "# 3. MAE in original score units (upvotes minus downvotes)\n",
    "baseline_mae_raw = mean_absolute_error(y_test_raw, y_test_pred_raw)\n",
    "\n",
    "print(f\"Baseline MAE (log space): {mean_absolute_error(y_test, y_test_pred_log):.4f}\")\n",
    "print(f\"Baseline MAE (raw score): {baseline_mae_raw:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 2: Predict Subreddit Mean with Global Mean fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test MAE  (log):  0.3484\n",
      "  Test MAE  (raw):  167.2744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ---- Baseline 2: Subreddit mean (with fallback to global mean) ----\n",
    "\n",
    "# 1. Compute mean *log-score* per subreddit on TRAIN ONLY\n",
    "train_df_for_means = pd.DataFrame(\n",
    "    {\n",
    "        \"subreddit\": X_train[\"subreddit\"].values,\n",
    "        \"score_log\": y_train,  # y_train is already log-transformed\n",
    "    }\n",
    ")\n",
    "\n",
    "subreddit_means_log = train_df_for_means.groupby(\"subreddit\")[\"score_log\"].mean()\n",
    "\n",
    "# Global mean in log space (used as fallback for unseen subreddits)\n",
    "global_mean_log = y_train.mean()\n",
    "\n",
    "\n",
    "def predict_subreddit_mean_log(X, subreddit_means_log, global_mean_log):\n",
    "    \"\"\"\n",
    "    For each row in X, predict the mean *log-score* of its subreddit,\n",
    "    falling back to the global mean (log) if the subreddit was unseen in training.\n",
    "    \"\"\"\n",
    "    return X[\"subreddit\"].map(subreddit_means_log).fillna(global_mean_log).values\n",
    "\n",
    "\n",
    "# 2. Predictions on TEST set (log space)\n",
    "y_test_pred_log = predict_subreddit_mean_log(\n",
    "    X_test, subreddit_means_log, global_mean_log\n",
    ")\n",
    "\n",
    "# 3. Metrics in LOG space\n",
    "mae_log = mean_absolute_error(y_test, y_test_pred_log)\n",
    "\n",
    "# 4. Inverse transform both true and predicted back to RAW score space\n",
    "# recall: y_log = log(score - min_score + 1)\n",
    "y_test_raw = np.exp(y_test) + min_score - 1\n",
    "y_test_pred_raw = np.exp(y_test_pred_log) + min_score - 1\n",
    "\n",
    "mae_raw = mean_absolute_error(y_test_raw, y_test_pred_raw)\n",
    "\n",
    "print(f\"  Test MAE  (log):  {mae_log:.4f}\")\n",
    "print(f\"  Test MAE  (raw):  {mae_raw:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with TF-IDF + Metadata Featuers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test MAE  (log): 0.3740\n",
      "  Test MAE  (raw): 179.6511\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "RANDOM_STATE = 42  # for CV shuffling\n",
    "\n",
    "lin_feature_cols = text_features + cat_features + num_features\n",
    "\n",
    "X_train_lin = X_train[lin_feature_cols].copy()\n",
    "X_test_lin = X_test[lin_feature_cols].copy()\n",
    "\n",
    "text_col = \"title\"\n",
    "cat_cols = cat_features\n",
    "num_cols = num_features\n",
    "\n",
    "preprocessor_lin = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"text\",\n",
    "            TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=5),\n",
    "            text_col,\n",
    "        ),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "            cat_cols,\n",
    "        ),\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            num_cols,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "lin_model = LinearRegression()\n",
    "\n",
    "lin_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor_lin),\n",
    "        (\"model\", lin_model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5 fold CV for consistency\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    lin_pipeline,\n",
    "    X_train_lin,\n",
    "    y_train,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "\n",
    "lin_pipeline.fit(X_train_lin, y_train)\n",
    "\n",
    "# Predictions in log space\n",
    "y_test_pred_log_lin = lin_pipeline.predict(X_test_lin)\n",
    "\n",
    "# Log-space metrics\n",
    "lin_mae_log = mean_absolute_error(y_test, y_test_pred_log_lin)\n",
    "\n",
    "# Inverse-transform back to raw score space\n",
    "y_test_raw = np.exp(y_test) + min_score - 1\n",
    "y_test_pred_raw_lin = np.exp(y_test_pred_log_lin) + min_score - 1\n",
    "\n",
    "lin_mae_raw = mean_absolute_error(y_test_raw, y_test_pred_raw_lin)\n",
    "\n",
    "print(f\"  Test MAE  (log): {lin_mae_log:.4f}\")\n",
    "print(f\"  Test MAE  (raw): {lin_mae_raw:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Champion Model: Ridge Regression with TF-IDF + Metadata Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Ridge MAE (log): 0.3383509792545443\n",
      "Ridge MAE (raw): 164.29552619735585\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "ridge_feature_cols = text_features + cat_features + num_features\n",
    "\n",
    "X_train_ridge = X_train[ridge_feature_cols].copy()\n",
    "X_test_ridge = X_test[ridge_feature_cols].copy()\n",
    "\n",
    "\n",
    "text_col = \"title\"\n",
    "cat_cols = cat_features\n",
    "num_cols = num_features\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"text\",\n",
    "            TfidfVectorizer(\n",
    "                max_features=20000,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=5\n",
    "            ),\n",
    "            text_col,\n",
    "        ),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "            cat_cols,\n",
    "        ),\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            num_cols,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5 fold CV\n",
    "base_ridge = Ridge(random_state=RANDOM_STATE)\n",
    "\n",
    "ridge_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", base_ridge),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"model__alpha\": [0.01, 0.1, 1, 3, 10, 30, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=ridge_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_absolute_error\",   #!!!! MAE in LOG space\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# y_train is log-transformed already!!!!! dont forget \n",
    "grid.fit(X_train_ridge, y_train)\n",
    "\n",
    "\n",
    "best_ridge = grid.best_estimator_\n",
    "\n",
    "# Predict in log(shifted score) space\n",
    "y_test_pred_log = best_ridge.predict(X_test_ridge)\n",
    "# Metrics in LOG space\n",
    "mae_log = mean_absolute_error(y_test, y_test_pred_log)\n",
    "\n",
    "y_test_pred_log = best_ridge.predict(X_test_ridge)\n",
    "# log metrics\n",
    "mae_log  = mean_absolute_error(y_test, y_test_pred_log)\n",
    "# back to raw\n",
    "y_test_raw      = np.exp(y_test)      + min_score - 1\n",
    "y_test_pred_raw = np.exp(y_test_pred_log) + min_score - 1\n",
    "mae_raw  = mean_absolute_error(y_test_raw, y_test_pred_raw)\n",
    "print(\"Ridge MAE (log):\", mae_log)\n",
    "print(\"Ridge MAE (raw):\", mae_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argument and Defense for Ridge:\n",
    "\n",
    "1. Theoretically most appropritate model\n",
    "    - Given that this is very high dimensional with TF-IDF features, this would not be a tree based problem (Random Forest, XGBoost)\n",
    "\n",
    "2. I guess academically most academically appropriate model, its standard in ML \n",
    "\n",
    "3. Better preformance compared to linear regresion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Visualizations\n",
    "\n",
    "Below we visualize the performance of our final Ridge Regression model:\n",
    "- Predicted vs Actual Scores\n",
    "- Residual Distribution\n",
    "- Residuals vs Predicted\n",
    "- Feature Importance (Ridge Coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# # Plot 1: Predicted vs Actual\n",
    "# axes[0, 0].scatter(y_true, y_pred, alpha=0.3)\n",
    "# axes[0, 0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \"r--\")\n",
    "# axes[0, 0].set_title(\"Predicted vs Actual\")\n",
    "# # This plot compares the model’s predicted Reddit scores to the actual scores.\n",
    "# # If the model were perfect, all points would fall on the diagonal line.\n",
    "# # Clustering tightly around the line indicates strong predictive performance,\n",
    "# # while spread indicates prediction error\n",
    "\n",
    "\n",
    "# # Plot 2: Residual Histogram\n",
    "# axes[0, 1].hist(residuals, bins=40, edgecolor=\"k\")\n",
    "# axes[0, 1].set_title(\"Residual Distribution\")\n",
    "# # This histogram shows the distribution of residuals (actual − predicted).\n",
    "# # A centered, symmetric shape around zero indicates that the model does not\n",
    "# # systematically over- or under-predict.\n",
    "# # Wide spread or skewness would indicate consistent bias.”\n",
    "\n",
    "\n",
    "# # Plot 3: Residuals vs Predicted\n",
    "# axes[1, 0].scatter(y_pred, residuals, alpha=0.3)\n",
    "# axes[1, 0].axhline(0, color=\"red\")\n",
    "# axes[1, 0].set_title(\"Residuals vs Predicted\")\n",
    "\n",
    "# # This visualization checks whether the model’s error changes depending on the\n",
    "# # predicted value. Ideally, residuals should be randomly scattered around zero.\n",
    "# # Patterns or funnels would suggest the model struggles at specific score ranges.\n",
    "\n",
    "\n",
    "# # Plot 4: Feature Importance (Top 20)\n",
    "# axes[1, 1].barh(top.index, top.values)\n",
    "# axes[1, 1].set_title(\"Feature Importance (Top 20)\")\n",
    "# axes[1, 1].invert_yaxis()\n",
    "\n",
    "# # Using Ridge Regression, we can measure which features have the strongest\n",
    "# # influence on predicting post score.\n",
    "# # Higher absolute coefficient values indicate more important features.\n",
    "# # This helps us understand which metadata and text patterns drive Reddit engagement.\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluated four models using mean absolute error (MAE) in both log-transformed and raw score space (Table X). The global-mean baseline achieved a log-MAE of 0.3695 (raw MAE = 174.5). Incorporating subreddit identity substantially improved performance: the per-subreddit mean baseline reduced log-MAE to 0.3479 and raw MAE to 167.0, indicating that subreddit choice is the strongest single predictor of score in our dataset.\n",
    "\n",
    "A plain Linear Regression model performed worse than both baselines (log-MAE = 0.3763; raw MAE = 180.6), likely due to overfitting the high-dimensional TF-IDF features. In contrast, a regularized Ridge Regression model achieved the best performance overall. After cross-validated tuning of the regularization parameter α, Ridge achieved a log-MAE of 0.3391 and a raw MAE of 164.6. Although the absolute improvement over the subreddit-mean baseline is modest, it demonstrates that textual, temporal, and sentiment features contribute incremental predictive signal beyond subreddit alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
